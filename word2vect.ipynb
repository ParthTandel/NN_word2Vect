{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import collections\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import os\n",
    "\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.getcwd()\n",
    "LOG_DIR = PATH + '/log'\n",
    "metadata = os.path.join(LOG_DIR, 'metadata.tsv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/parth/work_files/summer-2018/word2vect_python/log/metadata.tsv'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer_ = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'up', 'de', 'mostly', 'any', 'wherein', 'again', 'but', 'our', 'is', 'everywhere', 'thick', 'front', 'or', 'amongst', 'across', 'back', 'hundred', 'me', 'no', 'system', 'mill', 'per', 'throughout', 'at', 'whatever', 'then', 'third', 'still', 'once', 'empty', 'with', 'hasnt', 'its', 'must', 'next', 'behind', 'always', 'beforehand', 'keep', 're', 'had', 'sincere', 'being', 'latter', 'wherever', 'also', 'else', 'will', 'because', 'take', 'thereafter', 'yourselves', 'these', 'last', 'were', 'co', 'afterwards', 'how', 'part', 'often', 'rather', 'whom', 'seem', 'found', 'five', 'another', 'former', 'if', 'around', 'can', 'eleven', 'formerly', 'whereupon', 'this', 'first', 'yourself', 'four', 'such', 'of', 'much', 'my', 'three', 'be', 'detail', 'nevertheless', 'he', 'not', 'whence', 'who', 'yours', 'more', 'indeed', 'side', 'there', 'nothing', 'off', 'each', 'cant', 'somehow', 'whether', 'anyhow', 'un', 'a', 'we', 'cannot', 'down', 'by', 'yet', 'ourselves', 'would', 'cry', 'full', 'most', 'very', 'since', 'herein', 'into', 'sixty', 'from', 'someone', 'upon', 'along', 'among', 'should', 'thereupon', 'meanwhile', 'i', 'please', 'against', 'so', 'been', 'whereafter', 'ever', 'even', 'for', 'two', 'others', 'already', 'their', 'done', 'it', 'least', 'perhaps', 'alone', 'his', 'many', 'some', 'through', 'couldnt', 'name', 'whereby', 'which', 'her', 'am', 'have', 'everything', 'noone', 'anything', 'move', 'your', 'thus', 'than', 'might', 'where', 'within', 'toward', 'itself', 'amoungst', 'go', 'over', 'less', 'whereas', 'latterly', 'fire', 'either', 'whither', 'now', 'both', 'via', 'herself', 'do', 'hence', 'put', 'though', 'those', 'otherwise', 'twelve', 'ltd', 'neither', 'that', 'what', 'become', 'before', 'made', 'see', 'thru', 'few', 'under', 'none', 'nor', 'several', 'may', 'although', 'every', 'in', 'all', 'almost', 'becomes', 'bill', 'on', 'moreover', 'becoming', 'beside', 'somewhere', 'therefore', 'above', 'therein', 'well', 'was', 'own', 'con', 'myself', 'below', 'bottom', 'about', 'sometimes', 'seemed', 'eg', 'inc', 'anyone', 'seeming', 'out', 'the', 'has', 'them', 'further', 'sometime', 'forty', 'fill', 'hereupon', 'due', 'towards', 'whoever', 'interest', 'whose', 'anywhere', 'amount', 'give', 'eight', 'thence', 'twenty', 'together', 'other', 'fifty', 'except', 'ten', 'to', 'beyond', 'however', 'without', 'between', 'whenever', 'everyone', 'themselves', 'himself', 'when', 'became', 'during', 'etc', 'ie', 'thereby', 'here', 'you', 'show', 'besides', 'as', 'call', 'an', 'nowhere', 'onto', 'same', 'top', 'why', 'anyway', 'nobody', 'fifteen', 'never', 'one', 'thin', 'until', 'seems', 'six', 'describe', 'while', 'and', 'him', 'namely', 'whole', 'hereby', 'enough', 'find', 'us', 'could', 'hers', 'after', 'elsewhere', 'mine', 'nine', 'ours', 'serious', 'she', 'something', 'get', 'only', 'too', 'they', 'hereafter', 'are'})\n"
     ]
    }
   ],
   "source": [
    "print(stop_words.ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = glob.glob('MorpedTextedWWO/*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "newFullText  = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in paths:\n",
    "    with open(path, \"r\") as f:\n",
    "        newFullText += f.read().split(\"\\n\")[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "newFullText = newFullText.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 90000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_all = newFullText.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_all = [wordnet_lemmatizer.lemmatize(w) for w in words_all if w not in stop_words.ENGLISH_STOP_WORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124951"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(words_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6758697"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words, n_words):\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        index = dictionary.get(word, 0)\n",
    "        if index == 0:\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, count, dictionary, reverse_dictionary = build_dataset(words_all, vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_index = 0\n",
    "def createBatch(size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    batch = np.ndarray(shape=(size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1 # [ skip_window target skip_window ]\n",
    "    buffer = collections.deque(maxlen=span) # pylint: disable=redefined-builtin    \n",
    "    \n",
    "    buffer.extend(data[data_index:data_index + span])\n",
    "    data_index += span\n",
    "    \n",
    "    for i in range(size // num_skips):\n",
    "        context_words = [w for w in range(span) if w != skip_window]\n",
    "        words_to_use = random.sample(context_words, num_skips)\n",
    "        for j, context_word in enumerate(words_to_use):\n",
    "            \n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[context_word]\n",
    "            \n",
    "        if data_index == len(data):\n",
    "            buffer.extend(data[0:span])\n",
    "            data_index = span\n",
    "        else:\n",
    "            buffer.append(data[data_index])\n",
    "            data_index += 1\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(metadata, 'w') as metadata_file:\n",
    "    for i in range(vocabulary_size):\n",
    "        metadata_file.write(reverse_dictionary[i] + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "del words_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "del newFullText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "embedding_size = 128\n",
    "skip_window = 1\n",
    "num_skips = 2\n",
    "num_sampled = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_size = 16  # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-25-ff69b9516d24>:34: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.name_scope('inputs'):\n",
    "        train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "        train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "        valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "        \n",
    "        \n",
    "    with tf.device('/cpu:0'):\n",
    "        with tf.name_scope('embeddings'):\n",
    "            embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "        embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "        with tf.name_scope('weights'):\n",
    "            nce_weights = tf.Variable(\n",
    "                  tf.truncated_normal(\n",
    "                  [vocabulary_size, embedding_size],\n",
    "                  stddev=1.0 / math.sqrt(embedding_size)))\n",
    "        with tf.name_scope('biases'):\n",
    "            nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "        \n",
    "        with tf.name_scope('loss'):\n",
    "            loss = tf.reduce_mean(\n",
    "                tf.nn.nce_loss(\n",
    "                    weights=nce_weights,\n",
    "                    biases=nce_biases,\n",
    "                    labels=train_labels,\n",
    "                    inputs=embed,\n",
    "                    num_sampled=num_sampled,\n",
    "                    num_classes=vocabulary_size))\n",
    "        with tf.name_scope('optimizer'):\n",
    "            optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "        normalized_embeddings = embeddings / norm\n",
    "        valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "        similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "        init = tf.global_variables_initializer()\n",
    "        \n",
    "        saver = tf.train.Saver()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest to old: misteris, uninstructed, forwardness, presered, unpardonable, nave, transfused, manyfolde,\n",
      "Nearest to hath: attrocious, crease, inficit, tassel, culling, sod, ainslie, phidias,\n",
      "Nearest to mr.: unquiets, elogies, mediate, tamely, modifying, hollis, ghospell, reackoninge,\n",
      "Nearest to thing: ewells, vortigern, fidentia's, boulogne, boddington, sluice, grunt, hog-grubber,\n",
      "Nearest to country: schedule, odedience, cinchona, quiescit, clamav, unshipwrecked, impalement, trismegiste,\n",
      "Nearest to thy: nuptialls, champaigne, snipy, tyl, sanballet, finally, wall-creeper, preare,\n",
      "Nearest to tell: subjectes, wiggins, redress, lay-man, tenderhearted, blame, romantick, accrues,\n",
      "Nearest to dear: agréeing, buzing, buckinghamshire, mezentius, benefyghtes, th'immortal, hyacinthus, carelesness,\n",
      "Nearest to it's: accad, jav'lin, spill, cheerless, piggior, skinny, proportion, over-acted,\n",
      "Nearest to young: uzzah, hue, minysshe, wheatear, animall, bottomless, illustrata, landskip-tapistry,\n",
      "Nearest to u: watring, breath, travaile, blast's, elynes, saybrook, mercy-seat, parasytes,\n",
      "Nearest to life: significantly, glib, discordant, battle-shock, above-mentioned, containeth, wofull'st, legge,\n",
      "Nearest to person: newport, quipos's, belfery, comprise, andromache, themsel, openlye, neq,\n",
      "Nearest to come: potatoe-crop, saxe, refreshe, unprincipled, establishes, listener's, claymed, nwillingnesse,\n",
      "Nearest to nature: wiggins, cared, raphaël, domestick, ravished, disarranged, unmeasurable, rubricks,\n",
      "Nearest to men: corpes, full-grown-hopes, drooping, took's, mantinia, lothed, op'ning, compassyon,\n",
      "Average loss at step  2000 :  143.86665727424622\n",
      "Average loss at step  4000 :  82.28621610832214\n",
      "Average loss at step  6000 :  56.96339408349991\n",
      "Average loss at step  8000 :  42.82839484167099\n",
      "Average loss at step  10000 :  31.8004720569849\n",
      "Nearest to old: ―, scarcely, ;, pretty, wheat-ear, fearfull, act, strait,\n",
      "Nearest to hath: ,, gallic, hunted, sod, firme, baxter, change, beautious,\n",
      "Nearest to mr.: glorious, parnassus, saw, impressed, guess, light, sir, proo,\n",
      "Nearest to thing: fraser, biancha, boulogne, houre, way, set, suppress, desirous,\n",
      "Nearest to country: wheat-ear, trumpet, wenching, pardon, agent, bashful, e, 4.,\n",
      "Nearest to thy: teacher, ;, threatned, spain, sit, becca-fica, disliked, cultivated,\n",
      "Nearest to tell: ross, redress, blame, rein, shore, told, moistened, transformed,\n",
      "Nearest to dear: fair, miracle, slowly, buzing, news, tint, clotorindus, summit,\n",
      "Nearest to it's: cheerless, proportion, mackenzie, cruttwell, invoke, gray's-inn, accused, cave,\n",
      "Nearest to young: hue, ,, successe, doubted, know, ;, rushing, expressed,\n",
      "Nearest to u: becca-fica, instruction, hide, breath, j, bookseller, narmd, thank,\n",
      "Nearest to life: mackenzie, discordant, wofull'st, bloud, basket, obedience, ross, alarmed,\n",
      "Nearest to person: star, themsel, henceforth, love's, gifford, uncertain, meantime, wheat-ear,\n",
      "Nearest to come: promised, set, requite, yonder, gifford, cure, stop, tombe,\n",
      "Nearest to nature: ravished, cared, forget, miss, ross, priv, lock, carthage,\n",
      "Nearest to men: harvey, drooping, beseeching, disquiet, comming, ross, profit, difference,\n",
      "Average loss at step  12000 :  29.6206792191267\n",
      "Average loss at step  14000 :  24.659471981406213\n",
      "Average loss at step  16000 :  22.242614685297013\n",
      "Average loss at step  18000 :  18.100722556233407\n",
      "Average loss at step  20000 :  14.29219159913063\n",
      "Nearest to old: nave, ―, fearfull, forwardness, early, m‘intosh, scarcely, pretty,\n",
      "Nearest to hath: ;, saye, gallic, firme, hunted, attractive, cape, gy,\n",
      "Nearest to mr.: sir, glorious, tamely, patterson, guess, claimed, suburb, cryptogamia,\n",
      "Nearest to thing: fraser, boulogne, physiognomy, biancha, jesus, yorke, suppress, embellished,\n",
      "Nearest to country: wheat-ear, agent, trumpet, bashful, pardon, heretyke, albeit, ing,\n",
      "Nearest to thy: ;, hour, teacher, ,, outward, threatned, sit, steede,\n",
      "Nearest to tell: ross, redress, m‘intosh, blame, told, rein, elizabeth, foppish,\n",
      "Nearest to dear: su, fair, miracle, speciallie, slowly, buzing, tint, montague,\n",
      "Nearest to it's: proportion, cheerless, invoke, gray's-inn, mackenzie, accused, —, overtake,\n",
      "Nearest to young: hue, successe, under-rate, mocked, landskip-tapistry, deprecate, i.e., doubted,\n",
      "Nearest to u: becca-fica, christendome, instruction, piccadilly, subterraneous, anxiously, frolic, hide,\n",
      "Nearest to life: mackenzie, discordant, masonry, obedience, bloud, wofull'st, basket, ross,\n",
      "Nearest to person: newport, henceforth, star, themsel, uncertain, meantime, love's, nerena,\n",
      "Nearest to come: requite, inlargement, gifford, madrepore, promised, yonder, m‘intosh, plover,\n",
      "Nearest to nature: ravished, cared, forget, lock, miss, d'ye, ross, masse,\n",
      "Nearest to men: beseeching, comming, harvey, denotes, ;, ross, corpes, :,\n",
      "Average loss at step  22000 :  13.728090715289117\n",
      "Average loss at step  24000 :  12.033994528770446\n",
      "Average loss at step  26000 :  11.679469782829285\n",
      "Average loss at step  28000 :  10.846496794223786\n",
      "Average loss at step  30000 :  10.221413516759872\n",
      "Nearest to old: nave, ―, act, early, proclamation, fearfull, forwardness, pretty,\n",
      "Nearest to hath: ,, ;, carolinian, gallic, culling, innovation, callest, change,\n",
      "Nearest to mr.: mediate, sir, patterson, impressed, glorious, tamely, cherish, guess,\n",
      "Nearest to thing: fraser, belcher, glynne, dong, boulogne, way, —, loveday,\n",
      "Nearest to country: belcher, wheat-ear, agent, trumpet, bashful, ,, albeit, faythe,\n",
      "Nearest to thy: ;, —, :, ,, whitehead, hour, edmund, teacher,\n",
      "Nearest to tell: ross, m‘intosh, redress, told, blame, spooner, foppish, transformed,\n",
      "Nearest to dear: su, fair, montague, miracle, speciallie, pag, buzing, swash,\n",
      "Nearest to it's: cheerless, proportion, mackenzie, skinny, invoke, amory, overtake, accused,\n",
      "Nearest to young: hue, successe, under-rate, ,, thanksgiving, mocked, rest, deprecate,\n",
      "Nearest to u: becca-fica, christendome, instruction, subterraneous, belcher, piccadilly, bookseller, anxiously,\n",
      "Nearest to life: mackenzie, bloud, masonry, glynne, obedience, discordant, loveday, wofull'st,\n",
      "Nearest to person: newport, henceforth, meantime, star, uncertain, themsel, nepenthe, wheat-ear,\n",
      "Nearest to come: dong, requite, —, gifford, m‘intosh, set, recover, fabulous,\n",
      "Nearest to nature: ravished, loveday, cared, dong, domestick, miss, ross, enthusiast,\n",
      "Nearest to men: spooner, dong, cocked, harvey, ross, sentiment, beseeching, denotes,\n",
      "Average loss at step  32000 :  9.372840185046195\n",
      "Average loss at step  34000 :  8.29251705276966\n",
      "Average loss at step  36000 :  8.6690183198452\n",
      "Average loss at step  38000 :  7.476338653087616\n",
      "Average loss at step  40000 :  7.828797128796578\n",
      "Nearest to old: uninstructed, ―, nave, forwardness, early, act, pretty, proclamation,\n",
      "Nearest to hath: ,, ;, gallic, carolinian, :, culling, churche, innovation,\n",
      "Nearest to mr.: mediate, sir, mrs., patterson, tamely, cherish, impressed, hollis,\n",
      "Nearest to thing: way, fraser, ,, ), dong, belcher, loveday, glynne,\n",
      "Nearest to country: belcher, wheat-ear, agent, haslewood, trumpet, bashful, UNK, faythe,\n",
      "Nearest to thy: ;, ,, :, —, edmund, wheat-ear, outward, feasting,\n",
      "Nearest to tell: ross, told, redress, m‘intosh, spooner, blame, transformed, foppish,\n",
      "Nearest to dear: su, fair, th'immortal, montague, miracle, speciallie, pag, bitterest,\n",
      "Nearest to it's: proportion, cheerless, mackenzie, skinny, amory, loveday, invoke, overtake,\n",
      "Nearest to young: hue, successe, under-rate, thanksgiving, bottomless, mocked, doubted, deprecate,\n",
      "Nearest to u: becca-fica, christendome, abisse, subterraneous, belcher, UNK, instruction, eeles,\n",
      "Nearest to life: mackenzie, dong, masonry, obedience, glynne, bloud, ,, loveday,\n",
      "Nearest to person: newport, henceforth, meantime, star, mind, wheat-ear, uncertain, wheatley,\n",
      "Nearest to come: dong, gifford, UNK, relying, set, m‘intosh, requite, ,,\n",
      "Nearest to nature: ravished, loveday, cared, dong, ross, lock, enthusiast, m‘intosh,\n",
      "Nearest to men: spooner, ;, dong, sentiment, spirit, man, ,, harvey,\n",
      "Average loss at step  42000 :  7.52136900639534\n",
      "Average loss at step  44000 :  8.022541499257088\n",
      "Average loss at step  46000 :  7.159058326482773\n",
      "Average loss at step  48000 :  7.074044371247291\n",
      "Average loss at step  50000 :  7.075691452264786\n",
      "Nearest to old: uninstructed, nave, forwardness, young, act, early, pretty, proclamation,\n",
      "Nearest to hath: ;, :, culling, gallic, ,, carolinian, churche, doe,\n",
      "Nearest to mr.: sir, mrs., mediate, tamely, prolong, UNK, patterson, naval,\n",
      "Nearest to thing: way, ), fraser, dong, love, loveday, woman, belcher,\n",
      "Nearest to country: belcher, wheat-ear, agent, UNK, haslewood, trumpet, bashful, signer,\n",
      "Nearest to thy: ;, ,, :, ., doe, edmund, ?, wheat-ear,\n",
      "Nearest to tell: ross, told, say, m‘intosh, redress, spooner, ?, transformed,\n",
      "Nearest to dear: fair, su, th'immortal, UNK, bitterest, montague, admonished, speciallie,\n",
      "Nearest to it's: eighty-two, cheerless, proportion, skinny, mackenzie, hypericum, spill, amory,\n",
      "Nearest to young: hue, successe, old, under-rate, deprecate, bottomless, plump, spooner,\n",
      "Nearest to u: becca-fica, christendome, abisse, subterraneous, newt, piccadilly, eeles, belcher,\n",
      "Nearest to life: mackenzie, obedience, masonry, cordes, dong, glynne, loveday, time,\n",
      "Nearest to person: newport, mind, henceforth, wheatley, meantime, wheat-ear, uncertain, newt,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest to come: dong, UNK, —, gifford, requite, set, relying, ?,\n",
      "Nearest to nature: ravished, loveday, dong, cared, ross, m‘intosh, masse, wheatley,\n",
      "Nearest to men: man, spooner, dong, woman, sentiment, ;, eighty-two, spirit,\n",
      "Average loss at step  52000 :  7.20981882417202\n",
      "Average loss at step  54000 :  6.109973956108093\n",
      "Average loss at step  56000 :  6.30393492603302\n",
      "Average loss at step  58000 :  6.808628868103027\n",
      "Average loss at step  60000 :  7.090895910978317\n",
      "Nearest to old: nave, young, uninstructed, forwardness, act, early, pretty, m‘intosh,\n",
      "Nearest to hath: culling, (, carolinian, gallic, churche, glynne, perfection, innovation,\n",
      "Nearest to mr.: sir, mrs., tamely, UNK, mediate, ., naval, cherish,\n",
      "Nearest to thing: way, ), fraser, woman, dong, belcher, man, let,\n",
      "Nearest to country: belcher, wheat-ear, agent, metcalfe, haslewood, signer, UNK, bashful,\n",
      "Nearest to thy: ,, :, ;, god, becca-fica, doe, thee, wheat-ear,\n",
      "Nearest to tell: say, told, ross, m‘intosh, spooner, redress, transformed, romantick,\n",
      "Nearest to dear: fair, su, th'immortal, UNK, bitterest, said, admonished, speciallie,\n",
      "Nearest to it's: trower, eighty-two, bedlow, cheerless, skinny, hypericum, mackenzie, loveday,\n",
      "Nearest to young: hue, old, successe, under-rate, deprecate, terræ-filius, bowyer, agitated,\n",
      "Nearest to u: becca-fica, christendome, abisse, ―, subterraneous, newt, piccadilly, eeles,\n",
      "Nearest to life: mackenzie, trower, dong, obedience, glynne, cordes, loveday, time,\n",
      "Nearest to person: mind, newport, wheatley, henceforth, love, meantime, newt, methuen,\n",
      "Nearest to come: dong, —, set, bowyer, came, gifford, requite, return,\n",
      "Nearest to nature: ravished, loveday, ravensworth, kind, m‘intosh, masse, dong, ross,\n",
      "Nearest to men: man, woman, dong, spooner, eighty-two, sentiment, bowyer, spirit,\n",
      "Average loss at step  62000 :  6.095663431525231\n",
      "Average loss at step  64000 :  5.559392230510712\n",
      "Average loss at step  66000 :  5.840205278992653\n",
      "Average loss at step  68000 :  6.308729383111\n",
      "Average loss at step  70000 :  6.2212654671669005\n",
      "Nearest to old: young, nave, uninstructed, like, forwardness, act, UNK, early,\n",
      "Nearest to hath: kidd, ,, culling, carolinian, gallic, innovation, churche, ;,\n",
      "Nearest to mr.: sir, mrs., UNK, mediate, tamely, naval, ―, miss,\n",
      "Nearest to thing: way, woman, ), man, fraser, dong, opportunity, m‘intosh,\n",
      "Nearest to country: belcher, wheat-ear, UNK, agent, metcalfe, trumpet, haslewood, signer,\n",
      "Nearest to thy: :, becca-fica, ,, wheat-ear, whitehead, edmund, cunei, outward,\n",
      "Nearest to tell: told, say, ross, m‘intosh, spooner, redress, blame, romantick,\n",
      "Nearest to dear: fair, su, th'immortal, UNK, said, bitterest, admonished, montague,\n",
      "Nearest to it's: trower, eighty-two, bedlow, word, cheerless, ,, hypericum, proportion,\n",
      "Nearest to young: old, hue, under-rate, successe, deprecate, bowyer, terræ-filius, poor,\n",
      "Nearest to u: becca-fica, christendome, subterraneous, abisse, thee, eeles, UNK, ―,\n",
      "Nearest to life: kidd, mackenzie, trower, time, dong, love, glynne, cordes,\n",
      "Nearest to person: mind, newport, wheatley, love, woman, methuen, henceforth, man,\n",
      "Nearest to come: dong, came, return, UNK, set, bowyer, gifford, fabulous,\n",
      "Nearest to nature: ravished, loveday, m‘intosh, kind, dong, ross, masse, mind,\n",
      "Nearest to men: man, woman, dong, bowyer, eighty-two, spooner, ;, sentiment,\n",
      "Average loss at step  72000 :  6.288468615651131\n",
      "Average loss at step  74000 :  5.9188050445318225\n",
      "Average loss at step  76000 :  6.122771255493164\n",
      "Average loss at step  78000 :  6.029768453359604\n",
      "Average loss at step  80000 :  6.124181587696075\n",
      "Nearest to old: young, nave, like, uninstructed, pretty, forwardness, early, poor,\n",
      "Nearest to hath: kidd, culling, carolinian, :, ), gallic, ;, faculty,\n",
      "Nearest to mr.: sir, mrs., UNK, miss, tamely, mediate, ―, gladnesse,\n",
      "Nearest to thing: way, ), woman, fetherston, man, ,, m‘intosh, dong,\n",
      "Nearest to country: belcher, wheat-ear, place, agent, fetherston, haslewood, metcalfe, house,\n",
      "Nearest to thy: :, fetherston, god, detestation, whitehead, thee, wave, vanhattem,\n",
      "Nearest to tell: say, told, ross, m‘intosh, know, spooner, transformed, redress,\n",
      "Nearest to dear: fair, su, said, th'immortal, bitterest, admonished, UNK, wife,\n",
      "Nearest to it's: trower, eighty-two, bedlow, thing, that's, word, loveday, mackenzie,\n",
      "Nearest to young: old, hue, poor, under-rate, successe, bowyer, like, deprecate,\n",
      "Nearest to u: becca-fica, travaile, christendome, subterraneous, thee, abisse, narmd, eeles,\n",
      "Nearest to life: kidd, time, mackenzie, trower, love, dong, cordes, glynne,\n",
      "Nearest to person: mind, ,, lady, man, wheatley, woman, love, newport,\n",
      "Nearest to come: came, dong, return, fetherston, set, verite, brought, magnifique,\n",
      "Nearest to nature: ravished, kind, loveday, m‘intosh, mind, ,, dong, dress,\n",
      "Nearest to men: man, woman, fetherston, dong, bowyer, sentiment, people, thing,\n",
      "Average loss at step  82000 :  5.666579654216767\n",
      "Average loss at step  84000 :  6.025978853583336\n",
      "Average loss at step  86000 :  5.500357247591019\n",
      "Average loss at step  88000 :  6.363579661250115\n",
      "Average loss at step  90000 :  5.426457634329796\n",
      "Nearest to old: young, nave, like, wife, poor, UNK, uninstructed, act,\n",
      "Nearest to hath: kidd, harford, ,, culling, carolinian, ;, having, ),\n",
      "Nearest to mr.: mrs., sir, mediate, miss, UNK, tamely, gladnesse, lady,\n",
      "Nearest to thing: way, ), fetherston, harford, man, woman, m‘intosh, manner,\n",
      "Nearest to country: harford, wheat-ear, belcher, place, house, fetherston, UNK, metcalfe,\n",
      "Nearest to thy: fetherston, :, ,, god, ;, harford, doe, thee,\n",
      "Nearest to tell: say, told, harford, m‘intosh, ross, know, spooner, speak,\n",
      "Nearest to dear: said, fair, th'immortal, su, bitterest, poor, wife, admonished,\n",
      "Nearest to it's: trower, thing, eighty-two, bedlow, word, that's, loveday, —,\n",
      "Nearest to young: old, hue, poor, bowyer, under-rate, successe, deprecate, terræ-filius,\n",
      "Nearest to u: becca-fica, thee, travaile, christendome, subterraneous, UNK, abisse, narmd,\n",
      "Nearest to life: kidd, harford, time, present, trower, mackenzie, dong, love,\n",
      "Nearest to person: mind, lady, man, woman, men, love, thing, wheatley,\n",
      "Nearest to come: came, harford, dong, return, brought, set, fetherston, ,,\n",
      "Nearest to nature: kind, ravished, mind, m‘intosh, loveday, thing, dong, dress,\n",
      "Nearest to men: man, woman, people, fetherston, dong, bowyer, eighty-two, person,\n",
      "Average loss at step  92000 :  5.446977604627609\n",
      "Average loss at step  94000 :  5.150430377483368\n",
      "Average loss at step  96000 :  5.083339406967163\n",
      "Average loss at step  98000 :  5.2537208580970765\n",
      "Average loss at step  100000 :  5.185594229221344\n",
      "Nearest to old: young, nave, like, poor, UNK, pretty, forwardness, wife,\n",
      "Nearest to hath: kidd, harford, ), having, culling, dimsdale, carolinian, gallic,\n",
      "Nearest to mr.: mrs., sir, miss, mediate, tamely, lady, naval, suburb,\n",
      "Nearest to thing: way, ), woman, fetherston, harford, man, —, thought,\n",
      "Nearest to country: harford, wheat-ear, place, belcher, house, world, fetherston, agent,\n",
      "Nearest to thy: fetherston, wheat-ear, harford, doe, shall, :, ,, thee,\n",
      "Nearest to tell: say, told, harford, know, m‘intosh, ross, speak, spooner,\n",
      "Nearest to dear: said, fair, th'immortal, su, wife, poor, bitterest, admonished,\n",
      "Nearest to it's: trower, —, thing, eighty-two, that's, bedlow, thought, loveday,\n",
      "Nearest to young: old, poor, hue, successe, like, bowyer, under-rate, fair,\n",
      "Nearest to u: travaile, becca-fica, subterraneous, christendome, narmd, abisse, UNK, head,\n",
      "Nearest to life: kidd, time, harford, love, present, world, trower, dong,\n",
      "Nearest to person: woman, man, mind, lady, men, friend, love, thing,\n",
      "Nearest to come: came, harford, —, dong, brought, return, bring, fetherston,\n",
      "Nearest to nature: ravished, m‘intosh, kind, mind, loveday, dong, soul, eighty-two,\n",
      "Nearest to men: man, woman, people, fetherston, bowyer, dong, eighty-two, thing,\n"
     ]
    }
   ],
   "source": [
    "num_steps = 100001\n",
    "with tf.Session(graph=graph) as session:\n",
    "    init.run()\n",
    "    average_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batch_inputs, batch_labels = createBatch(batch_size, num_skips, skip_window)\n",
    "        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "        \n",
    "        _, loss_val = session.run(\n",
    "            [optimizer, loss],\n",
    "            feed_dict=feed_dict)\n",
    "        average_loss += loss_val\n",
    "        \n",
    "        if step % 2000 == 0 and step > 0:\n",
    "            average_loss /= 2000\n",
    "            print('Average loss at step ', step, ': ', average_loss)\n",
    "            average_loss = 0\n",
    "        \n",
    "        \n",
    "        if step % 10000 == 0:\n",
    "            sim = similarity.eval()\n",
    "            for i in range(valid_size):\n",
    "                valid_word = reverse_dictionary[valid_examples[i]]\n",
    "                top_k = 8\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                log_str = 'Nearest to %s:' % valid_word\n",
    "                for k in range(top_k):\n",
    "                    close_word = reverse_dictionary[nearest[k]]\n",
    "                    log_str = '%s %s,' % (log_str, close_word)\n",
    "                print(log_str)\n",
    "    final_embeddings = normalized_embeddings.eval()\n",
    "    \n",
    "    saver.save(session, os.path.join(LOG_DIR, 'model.ckpt'))\n",
    "\n",
    "    config = projector.ProjectorConfig()\n",
    "\n",
    "    embedding_conf = config.embeddings.add()\n",
    "    embedding_conf.tensor_name = embeddings.name\n",
    "    embedding_conf.metadata_path = metadata\n",
    "    projector.visualize_embeddings(tf.summary.FileWriter(LOG_DIR), config)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_words(word, final_embeddings, k=10):\n",
    "    word_id = dictionary[word]\n",
    "    similarities = cosine_similarity([final_embeddings[word_id]], final_embeddings)[0]\n",
    "    sort = np.argsort(similarities)[::-1]\n",
    "    probs = []\n",
    "    for x in sort[0:k]:\n",
    "        probs.append((similarities[x], reverse_dictionary[x]))\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0000002, 'founded'),\n",
       " (0.509346, ','),\n",
       " (0.4926116, 'harford'),\n",
       " (0.47872818, 'uninjured'),\n",
       " (0.47564012, 'porcelline'),\n",
       " (0.47105646, 'shall'),\n",
       " (0.4703773, 'vallies'),\n",
       " (0.4694081, 'tael'),\n",
       " (0.4671404, 'brav'),\n",
       " (0.46606153, 'bon-gre')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_n_words(\"founded\", final_embeddings, k=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
