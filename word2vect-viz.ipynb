{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import glob\n",
    "import collections\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction import stop_words\n",
    "import os\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "\n",
    "\n",
    "import sys\n",
    "import argparse\n",
    "import random\n",
    "from tempfile import gettempdir\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = os.path.dirname(os.path.realpath(sys.argv[0]))\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '--log_dir',\n",
    "    type=str,\n",
    "    default=os.path.join(current_path, 'log-tb-4-june'),\n",
    "    help='The log directory for TensorBoard summaries.')\n",
    "FLAGS, unparsed = parser.parse_known_args()\n",
    "\n",
    "# Create the directory for TensorBoard variables if there is not.\n",
    "if not os.path.exists(FLAGS.log_dir):\n",
    "    os.makedirs(FLAGS.log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths = glob.glob('MorpedTextedWWO/*.txt')\n",
    "paths = glob.glob('/home/parth/work_files/oxy-4june/www-plaintext/*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "newFullText  = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in paths:\n",
    "    with open(path, \"r\") as f:\n",
    "        newFullText += f.read().split(\"\\n\")[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "newFullText = newFullText.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_all = newFullText.split(\" \")\n",
    "# words_all = [w for w in words_all if w not in stop_words.ENGLISH_STOP_WORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words, n_words):\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        index = dictionary.get(word, 0)\n",
    "        if index == 0:\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, count, dictionary, reverse_dictionary = build_dataset(words_all, vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBatch(size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    batch = np.ndarray(shape=(size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1 # [ skip_window target skip_window ]\n",
    "    buffer = collections.deque(maxlen=span) # pylint: disable=redefined-builtin    \n",
    "    \n",
    "    buffer.extend(data[data_index:data_index + span])\n",
    "    data_index += span\n",
    "    \n",
    "    \n",
    "#     print(\"******************\")\n",
    "#     print(buffer)\n",
    "#     print(len(buffer))\n",
    "#     print(\"******************\")\n",
    "    \n",
    "    \n",
    "    for i in range(size // num_skips):\n",
    "        context_words = [w for w in range(span) if w != skip_window]\n",
    "        words_to_use = random.sample(context_words, num_skips)\n",
    "        for j, context_word in enumerate(words_to_use):\n",
    "            \n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[context_word]\n",
    "            \n",
    "            \n",
    "#             print(\"index\", buffer[skip_window], buffer[context_word])\n",
    "#             print(\"label\", skip_window, buffer[skip_window])\n",
    "\n",
    "        if data_index == len(data):\n",
    "            buffer.extend(data[0:span])\n",
    "            data_index = span\n",
    "        else:\n",
    "            buffer.append(data[data_index])\n",
    "            data_index += 1\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 144\n",
    "embedding_size = 128  # Dimension of the embedding vector.\n",
    "skip_window = 12  # How many words to consider left and right.\n",
    "num_skips = 12  # How many times to reuse an input to generate a label.\n",
    "num_sampled = 32  # Number of negative examples to sample.\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors. Here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent. These 3 variables are used only for\n",
    "# displaying model accuracy, they don't affect calculation.\n",
    "valid_size = 16  # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-912d5d634619>:46: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "    with tf.name_scope('inputs'):\n",
    "        train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "        train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "        valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "  # Ops and variables pinned to the CPU because of missing GPU implementation\n",
    "    with tf.device('/cpu:0'):\n",
    "        # Look up embeddings for inputs.\n",
    "        with tf.name_scope('embeddings'):\n",
    "            embeddings = tf.Variable(\n",
    "                tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "            embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "    # Construct the variables for the NCE loss\n",
    "    with tf.name_scope('weights'):\n",
    "        nce_weights = tf.Variable(\n",
    "            tf.truncated_normal(\n",
    "                [vocabulary_size, embedding_size],\n",
    "                stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    with tf.name_scope('biases'):\n",
    "        nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    with tf.name_scope('loss'):\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.nce_loss(\n",
    "                weights=nce_weights,\n",
    "                biases=nce_biases,\n",
    "                labels=train_labels,\n",
    "                inputs=embed,\n",
    "                num_sampled=num_sampled,\n",
    "                num_classes=vocabulary_size))\n",
    "\n",
    "    # Add the loss value as a scalar to summary.\n",
    "    tf.summary.scalar('loss', loss)\n",
    "\n",
    "    # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "    with tf.name_scope('optimizer'):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "\n",
    "  # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings,\n",
    "                                            valid_dataset)\n",
    "    similarity = tf.matmul(\n",
    "        valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "    # Merge all summaries.\n",
    "    merged = tf.summary.merge_all()\n",
    "\n",
    "    # Add variable initializer.\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Create a saver.\n",
    "    saver = tf.train.Saver()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_steps = 100001\n",
    "# data_index = 0\n",
    "# # for step in xrange(num_steps):\n",
    "# #     batch_inputs, batch_labels = createBatch(batch_size, num_skips,skip_window)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step  0 :  159.7100372314453\n",
      "Average loss at step  2000 :  75.73207667636872\n",
      "Average loss at step  4000 :  44.24920387470722\n",
      "Average loss at step  6000 :  40.730657796144484\n",
      "Average loss at step  8000 :  29.7687312605381\n",
      "Average loss at step  10000 :  22.3921632540226\n",
      "Average loss at step  12000 :  22.950083320736884\n",
      "Average loss at step  14000 :  19.369962785482407\n",
      "Average loss at step  16000 :  16.800360976099967\n",
      "Average loss at step  18000 :  14.167467841744424\n",
      "Average loss at step  20000 :  12.653556562304496\n",
      "Average loss at step  22000 :  14.23383607852459\n",
      "Average loss at step  24000 :  11.842587855458259\n",
      "Average loss at step  26000 :  10.93060578083992\n",
      "Average loss at step  28000 :  8.508837864398956\n",
      "Average loss at step  30000 :  8.953792636871338\n",
      "Average loss at step  32000 :  8.465025626540184\n",
      "Average loss at step  34000 :  8.310250946044922\n",
      "Average loss at step  36000 :  7.994609128594399\n",
      "Average loss at step  38000 :  7.730327393651009\n",
      "Average loss at step  40000 :  6.944639231085778\n",
      "Average loss at step  42000 :  6.597293213009834\n",
      "Average loss at step  44000 :  6.203346096158028\n",
      "Average loss at step  46000 :  5.84152097260952\n",
      "Average loss at step  48000 :  5.769920411705971\n",
      "Average loss at step  50000 :  5.648579127430915\n",
      "Average loss at step  52000 :  6.749704682350159\n",
      "Average loss at step  54000 :  6.716015580415726\n",
      "Average loss at step  56000 :  6.453980554461479\n",
      "Average loss at step  58000 :  6.413161670327186\n",
      "Average loss at step  60000 :  5.633684529900551\n",
      "Average loss at step  62000 :  6.382955697774887\n",
      "Average loss at step  64000 :  6.050465867638588\n",
      "Average loss at step  66000 :  5.7929082634449\n",
      "Average loss at step  68000 :  5.503030737757682\n",
      "Average loss at step  70000 :  6.139214100837708\n",
      "Average loss at step  72000 :  5.803773710489273\n",
      "Average loss at step  74000 :  5.55934281373024\n",
      "Average loss at step  76000 :  5.657155733823776\n",
      "Average loss at step  78000 :  5.651098594784736\n",
      "Average loss at step  80000 :  5.521116083741188\n",
      "Average loss at step  82000 :  5.18426393520832\n",
      "Average loss at step  84000 :  5.063123025894165\n",
      "Average loss at step  86000 :  5.230351980686188\n",
      "Average loss at step  88000 :  5.608335479736328\n",
      "Average loss at step  90000 :  5.392045330047607\n",
      "Average loss at step  92000 :  5.223781011581421\n",
      "Average loss at step  94000 :  5.265521438121795\n",
      "Average loss at step  96000 :  5.377882602930069\n",
      "Average loss at step  98000 :  5.356393563389778\n",
      "Average loss at step  100000 :  5.203091070771217\n",
      "Average loss at step  102000 :  5.086844702720642\n",
      "Average loss at step  104000 :  5.105758496761322\n",
      "Average loss at step  106000 :  4.909029536008835\n",
      "Average loss at step  108000 :  4.838568707346917\n",
      "Average loss at step  110000 :  4.91514552462101\n",
      "Average loss at step  112000 :  4.804504467606544\n",
      "Average loss at step  114000 :  4.8438692111969\n",
      "Average loss at step  116000 :  4.905850887894631\n",
      "Average loss at step  118000 :  4.74033684194088\n",
      "Average loss at step  120000 :  4.804880468845368\n",
      "Average loss at step  122000 :  4.766629881978035\n",
      "Average loss at step  124000 :  4.861378769695759\n",
      "Average loss at step  126000 :  4.830652099609375\n",
      "Average loss at step  128000 :  4.922367413043975\n",
      "Average loss at step  130000 :  4.91599512386322\n",
      "Average loss at step  132000 :  4.94436505484581\n",
      "Average loss at step  134000 :  4.940244255661964\n",
      "Average loss at step  136000 :  4.73187689769268\n",
      "Average loss at step  138000 :  4.640174959659577\n",
      "Average loss at step  140000 :  4.482541091799736\n",
      "Average loss at step  142000 :  4.827138360857964\n",
      "Average loss at step  144000 :  4.7571768485307695\n",
      "Average loss at step  146000 :  4.676095061063767\n",
      "Average loss at step  148000 :  4.558795260429382\n",
      "Average loss at step  150000 :  4.6332698241472245\n",
      "Average loss at step  152000 :  4.977116522550583\n",
      "Average loss at step  154000 :  4.86003062748909\n",
      "Average loss at step  156000 :  4.768863009810448\n",
      "Average loss at step  158000 :  4.776762670755386\n",
      "Average loss at step  160000 :  4.64581899535656\n",
      "Average loss at step  162000 :  4.552848197340965\n",
      "Average loss at step  164000 :  4.950760882019996\n",
      "Average loss at step  166000 :  4.715519444823265\n",
      "Average loss at step  168000 :  4.943390214204788\n",
      "Average loss at step  170000 :  4.247266485452652\n",
      "Average loss at step  172000 :  4.7337261747121815\n",
      "Average loss at step  174000 :  4.60030641913414\n",
      "Average loss at step  176000 :  4.7379012850523\n",
      "Average loss at step  178000 :  4.585065661072731\n",
      "Average loss at step  180000 :  4.745025581955909\n",
      "Average loss at step  182000 :  4.629728441357613\n",
      "Average loss at step  184000 :  4.551300372362137\n",
      "Average loss at step  186000 :  4.618500522494316\n",
      "Average loss at step  188000 :  4.489167770028114\n",
      "Average loss at step  190000 :  4.5224068571329115\n",
      "Average loss at step  192000 :  4.4487382185459134\n",
      "Average loss at step  194000 :  4.471014735102654\n",
      "Average loss at step  196000 :  4.475849930286407\n",
      "Average loss at step  198000 :  4.5031997663974765\n",
      "Average loss at step  200000 :  4.445230384588242\n",
      "Average loss at step  202000 :  4.428614503026009\n",
      "Average loss at step  204000 :  4.764343281626702\n",
      "Average loss at step  206000 :  4.622660403490067\n",
      "Average loss at step  208000 :  4.498890554785729\n",
      "Average loss at step  210000 :  4.45107666182518\n",
      "Average loss at step  212000 :  4.64491506755352\n",
      "Average loss at step  214000 :  4.536450977802277\n",
      "Average loss at step  216000 :  4.379834773421288\n",
      "Average loss at step  218000 :  4.40991991865635\n",
      "Average loss at step  220000 :  4.38221236884594\n",
      "Average loss at step  222000 :  4.626571686983109\n",
      "Average loss at step  224000 :  4.528907498717308\n",
      "Average loss at step  226000 :  4.505800314545631\n",
      "Average loss at step  228000 :  4.5168689606189725\n",
      "Average loss at step  230000 :  4.496088397622109\n",
      "Average loss at step  232000 :  4.445774890780449\n",
      "Average loss at step  234000 :  4.394782745599747\n",
      "Average loss at step  236000 :  4.509996075034142\n",
      "Average loss at step  238000 :  4.481083932757378\n",
      "Average loss at step  240000 :  4.585417040705681\n",
      "Average loss at step  242000 :  4.455307675600052\n",
      "Average loss at step  244000 :  4.413999406456948\n",
      "Average loss at step  246000 :  4.416275070667266\n",
      "Average loss at step  248000 :  4.491913106560707\n",
      "Average loss at step  250000 :  4.610665720462799\n",
      "Average loss at step  252000 :  4.514873448491096\n",
      "Average loss at step  254000 :  4.425760629534722\n",
      "Average loss at step  256000 :  4.516715733289718\n",
      "Average loss at step  258000 :  4.50058838224411\n",
      "Average loss at step  260000 :  4.4843745470047\n",
      "Average loss at step  262000 :  4.581395648479462\n",
      "Average loss at step  264000 :  4.408633546710014\n",
      "Average loss at step  266000 :  4.171811556100845\n",
      "Average loss at step  268000 :  4.572335416555405\n",
      "Average loss at step  270000 :  4.4496886106729505\n",
      "Average loss at step  272000 :  4.491373198270797\n",
      "Average loss at step  274000 :  4.592686032414436\n",
      "Average loss at step  276000 :  4.600231409668923\n",
      "Average loss at step  278000 :  4.44799765586853\n",
      "Average loss at step  280000 :  4.677346861600876\n",
      "Average loss at step  282000 :  5.652798024296761\n",
      "Average loss at step  284000 :  4.594171077847481\n",
      "Average loss at step  286000 :  4.519636245250702\n",
      "Average loss at step  288000 :  4.275492585539817\n",
      "Average loss at step  290000 :  4.392567990541458\n",
      "Average loss at step  292000 :  4.485796380281449\n",
      "Average loss at step  294000 :  4.366588798880577\n",
      "Average loss at step  296000 :  4.4242361816167834\n",
      "Average loss at step  298000 :  4.432982655882835\n",
      "Average loss at step  300000 :  4.4791315041780475\n",
      "Average loss at step  302000 :  4.4558155380487445\n",
      "Average loss at step  304000 :  4.394954123020172\n",
      "Average loss at step  306000 :  4.402988299548626\n",
      "Average loss at step  308000 :  4.50120863699913\n",
      "Average loss at step  310000 :  4.4849452419281\n",
      "Average loss at step  312000 :  4.320974926412106\n",
      "Average loss at step  314000 :  4.5848949307203295\n",
      "Average loss at step  316000 :  4.485226127266884\n",
      "Average loss at step  318000 :  4.438940806031227\n",
      "Average loss at step  320000 :  4.413646415948868\n",
      "Average loss at step  322000 :  4.503690656781196\n",
      "Average loss at step  324000 :  4.605050547838211\n",
      "Average loss at step  326000 :  4.477431701898575\n",
      "Average loss at step  328000 :  4.456793154001236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  330000 :  4.474761876106262\n",
      "Average loss at step  332000 :  4.543403644323349\n",
      "Average loss at step  334000 :  4.603809056997299\n",
      "Average loss at step  336000 :  4.475053770542145\n",
      "Average loss at step  338000 :  4.385532939434052\n",
      "Average loss at step  340000 :  4.351508023262024\n",
      "Average loss at step  342000 :  4.3140484303236\n",
      "Average loss at step  344000 :  4.275401655077935\n",
      "Average loss at step  346000 :  4.296652863502502\n",
      "Average loss at step  348000 :  4.252118652582168\n",
      "Average loss at step  350000 :  4.262676032066345\n",
      "Average loss at step  352000 :  4.264108455300331\n",
      "Average loss at step  354000 :  4.276390666365623\n",
      "Average loss at step  356000 :  4.235719795227051\n",
      "Average loss at step  358000 :  4.236018574357033\n",
      "Average loss at step  360000 :  4.25402494931221\n",
      "Average loss at step  362000 :  4.434459977388382\n",
      "Average loss at step  364000 :  4.401349250435829\n",
      "Average loss at step  366000 :  4.171463490962982\n",
      "Average loss at step  368000 :  4.123364877581596\n",
      "Average loss at step  370000 :  4.252601077198983\n",
      "Average loss at step  372000 :  4.482084829628468\n",
      "Average loss at step  374000 :  4.4974591022729875\n",
      "Average loss at step  376000 :  4.400298261642456\n",
      "Average loss at step  378000 :  4.262193631768227\n",
      "Average loss at step  380000 :  4.246300469279289\n",
      "Average loss at step  382000 :  4.318248335123062\n",
      "Average loss at step  384000 :  4.4787819237113\n",
      "Average loss at step  386000 :  4.255479591369629\n",
      "Average loss at step  388000 :  4.5429571554660795\n",
      "Average loss at step  390000 :  4.395025223612786\n",
      "Average loss at step  392000 :  4.4840707558393476\n",
      "Average loss at step  394000 :  4.31430166375637\n",
      "Average loss at step  396000 :  4.542989667892456\n",
      "Average loss at step  398000 :  4.292717742204666\n",
      "Average loss at step  400000 :  4.1530830388069155\n",
      "Average loss at step  402000 :  4.589613722324371\n",
      "Average loss at step  404000 :  4.307038459181785\n",
      "Average loss at step  406000 :  4.2824995791912075\n",
      "Average loss at step  408000 :  4.215570828676224\n",
      "Average loss at step  410000 :  4.135402819395066\n",
      "Average loss at step  412000 :  4.25666825401783\n",
      "Average loss at step  414000 :  4.431393700659275\n",
      "Average loss at step  416000 :  4.335570001840591\n",
      "Average loss at step  418000 :  4.300894780278206\n",
      "Average loss at step  420000 :  4.166348738908768\n",
      "Average loss at step  422000 :  4.346169496178627\n",
      "Average loss at step  424000 :  4.323722409248352\n",
      "Average loss at step  426000 :  4.409375011682511\n",
      "Average loss at step  428000 :  4.3955180068016055\n",
      "Average loss at step  430000 :  4.43260904455185\n",
      "Average loss at step  432000 :  4.461693350553513\n",
      "Average loss at step  434000 :  4.3789264600873\n",
      "Average loss at step  436000 :  4.385467269420624\n",
      "Average loss at step  438000 :  4.360152832627296\n",
      "Average loss at step  440000 :  4.295230507850647\n",
      "Average loss at step  442000 :  4.26562236905098\n",
      "Average loss at step  444000 :  4.235034183621407\n",
      "Average loss at step  446000 :  4.19460650742054\n",
      "Average loss at step  448000 :  4.337844041228294\n",
      "Average loss at step  450000 :  4.300026764154434\n",
      "Average loss at step  452000 :  4.347572121679783\n",
      "Average loss at step  454000 :  4.474511386156082\n",
      "Average loss at step  456000 :  4.3885535019636155\n",
      "Average loss at step  458000 :  4.327290570497513\n",
      "Average loss at step  460000 :  4.32434567540884\n",
      "Average loss at step  462000 :  4.217113631665707\n",
      "Average loss at step  464000 :  4.116516913890838\n",
      "Average loss at step  466000 :  4.0908315147161485\n",
      "Average loss at step  468000 :  4.144301674425602\n",
      "Average loss at step  470000 :  4.11201403927803\n",
      "Average loss at step  472000 :  4.171899249434471\n",
      "Average loss at step  474000 :  4.3366787423491475\n",
      "Average loss at step  476000 :  4.347634380459786\n",
      "Average loss at step  478000 :  4.441632736563682\n",
      "Average loss at step  480000 :  4.335124807357788\n",
      "Average loss at step  482000 :  4.39385435962677\n",
      "Average loss at step  484000 :  4.357604043602944\n",
      "Average loss at step  486000 :  4.248847187876701\n",
      "Average loss at step  488000 :  4.374365450263023\n",
      "Average loss at step  490000 :  4.258750041604042\n",
      "Average loss at step  492000 :  4.921106563329697\n",
      "Average loss at step  494000 :  4.999227270841598\n",
      "Average loss at step  496000 :  4.0762877863645555\n",
      "Average loss at step  498000 :  3.998436381340027\n",
      "Average loss at step  500000 :  4.234224837303161\n",
      "Average loss at step  502000 :  4.48381015086174\n",
      "Average loss at step  504000 :  4.420477244138717\n",
      "Average loss at step  506000 :  4.381453750729561\n",
      "Average loss at step  508000 :  4.39500799202919\n",
      "Average loss at step  510000 :  4.325289416432381\n",
      "Average loss at step  512000 :  4.439073763012886\n",
      "Average loss at step  514000 :  4.330318158864975\n",
      "Average loss at step  516000 :  4.28362089240551\n",
      "Average loss at step  518000 :  4.23290336561203\n",
      "Average loss at step  520000 :  4.320174118757248\n",
      "Average loss at step  522000 :  4.218454558849334\n",
      "Average loss at step  524000 :  4.41380481338501\n",
      "Average loss at step  526000 :  4.353994782090187\n",
      "Average loss at step  528000 :  4.423264557600022\n",
      "Average loss at step  530000 :  4.333406586527825\n",
      "Average loss at step  532000 :  3.7121706022620202\n",
      "Average loss at step  534000 :  4.491453335762024\n",
      "Average loss at step  536000 :  4.347135604143142\n",
      "Average loss at step  538000 :  4.307882364153862\n",
      "Average loss at step  540000 :  4.318980907559395\n",
      "Average loss at step  542000 :  4.331405041098595\n",
      "Average loss at step  544000 :  4.416583612203598\n",
      "Average loss at step  546000 :  4.385284043192863\n",
      "Average loss at step  548000 :  4.296664354085922\n",
      "Average loss at step  550000 :  4.284578250527382\n",
      "Average loss at step  552000 :  4.292794050335884\n",
      "Average loss at step  554000 :  4.204242691993714\n",
      "Average loss at step  556000 :  4.4626940242052076\n",
      "Average loss at step  558000 :  4.360732014358043\n",
      "Average loss at step  560000 :  4.375993518710136\n",
      "Average loss at step  562000 :  4.298505085349083\n",
      "Average loss at step  564000 :  4.371578026652336\n",
      "Average loss at step  566000 :  4.290911704421044\n",
      "Average loss at step  568000 :  4.402647125601768\n",
      "Average loss at step  570000 :  4.453589107275009\n",
      "Average loss at step  572000 :  4.257083926320076\n",
      "Average loss at step  574000 :  4.49749080491066\n",
      "Average loss at step  576000 :  4.397512086629868\n",
      "Average loss at step  578000 :  4.309909475684166\n",
      "Average loss at step  580000 :  4.30108574950695\n",
      "Average loss at step  582000 :  4.305661702156067\n",
      "Average loss at step  584000 :  4.3445724347829815\n",
      "Average loss at step  586000 :  4.347444072604179\n",
      "Average loss at step  588000 :  4.24299007999897\n",
      "Average loss at step  590000 :  4.313923726320267\n",
      "Average loss at step  592000 :  4.375116354584694\n",
      "Average loss at step  594000 :  4.319464120149612\n",
      "Average loss at step  596000 :  4.3862007724046705\n",
      "Average loss at step  598000 :  4.226147751450538\n",
      "Average loss at step  600000 :  4.383232879519462\n",
      "Average loss at step  602000 :  4.945246894717217\n",
      "Average loss at step  604000 :  4.32685109770298\n",
      "Average loss at step  606000 :  4.456353350639343\n",
      "Average loss at step  608000 :  4.40482603931427\n",
      "Average loss at step  610000 :  4.382863473415375\n",
      "Average loss at step  612000 :  4.446078308463097\n",
      "Average loss at step  614000 :  4.311459650754928\n",
      "Average loss at step  616000 :  4.408527267813683\n",
      "Average loss at step  618000 :  4.299653681516648\n",
      "Average loss at step  620000 :  4.282264521002769\n",
      "Average loss at step  622000 :  4.270473262548447\n",
      "Average loss at step  624000 :  4.27740408372879\n",
      "Average loss at step  626000 :  4.265593629479408\n",
      "Average loss at step  628000 :  4.14442625772953\n",
      "Average loss at step  630000 :  4.204694965243339\n",
      "Average loss at step  632000 :  4.395634985327721\n",
      "Average loss at step  634000 :  4.1903060678243635\n",
      "Average loss at step  636000 :  4.159005836606026\n",
      "Average loss at step  638000 :  4.458937066853046\n",
      "Average loss at step  640000 :  4.057782798528671\n",
      "Average loss at step  642000 :  4.209083389520645\n",
      "Average loss at step  644000 :  4.298007498502732\n",
      "Average loss at step  646000 :  4.3431772420406345\n",
      "Average loss at step  648000 :  4.41037390422821\n",
      "Average loss at step  650000 :  4.222074795186519\n",
      "Average loss at step  652000 :  4.338107443451881\n",
      "Average loss at step  654000 :  4.307409662246704\n",
      "Average loss at step  656000 :  4.257876374840737\n",
      "Average loss at step  658000 :  4.277610303997993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  660000 :  4.249952207684517\n",
      "Average loss at step  662000 :  4.305019678354263\n",
      "Average loss at step  664000 :  4.32931339263916\n",
      "Average loss at step  666000 :  4.318633353233337\n",
      "Average loss at step  668000 :  4.3252765306234355\n",
      "Average loss at step  670000 :  4.265398904442788\n",
      "Average loss at step  672000 :  4.346717289090156\n",
      "Average loss at step  674000 :  4.267895581305027\n",
      "Average loss at step  676000 :  4.179857656240463\n",
      "Average loss at step  678000 :  4.023809662103653\n",
      "Average loss at step  680000 :  4.0627296621799465\n",
      "Average loss at step  682000 :  3.961513548016548\n",
      "Average loss at step  684000 :  4.122569019913674\n",
      "Average loss at step  686000 :  8.161204714387654\n",
      "Average loss at step  688000 :  4.370270961999894\n",
      "Average loss at step  690000 :  4.259252187728882\n",
      "Average loss at step  692000 :  4.429004257380963\n",
      "Average loss at step  694000 :  4.355642389833927\n",
      "Average loss at step  696000 :  4.371821584224701\n",
      "Average loss at step  698000 :  4.287897708773613\n",
      "Average loss at step  700000 :  4.396662011146545\n",
      "Average loss at step  702000 :  4.302412742435932\n",
      "Average loss at step  704000 :  4.24745913541317\n",
      "Average loss at step  706000 :  4.188630340099334\n",
      "Average loss at step  708000 :  4.1440712360143666\n",
      "Average loss at step  710000 :  4.29054582118988\n",
      "Average loss at step  712000 :  4.255222320914268\n",
      "Average loss at step  714000 :  4.2495838253498075\n",
      "Average loss at step  716000 :  4.17805793106556\n",
      "Average loss at step  718000 :  4.338033626437187\n",
      "Average loss at step  720000 :  4.237355342566967\n",
      "Average loss at step  722000 :  4.37038695371151\n",
      "Average loss at step  724000 :  4.080736548900604\n",
      "Average loss at step  726000 :  4.234042896866798\n",
      "Average loss at step  728000 :  4.292581004500389\n",
      "Average loss at step  730000 :  4.378727816939354\n",
      "Average loss at step  732000 :  4.314639143228531\n",
      "Average loss at step  734000 :  4.344872791409492\n",
      "Average loss at step  736000 :  4.298029711365699\n",
      "Average loss at step  738000 :  4.064151733398438\n",
      "Average loss at step  740000 :  4.083617377758026\n",
      "Average loss at step  742000 :  3.963316469550133\n",
      "Average loss at step  744000 :  4.06381271982193\n",
      "Average loss at step  746000 :  4.370305151700974\n",
      "Average loss at step  748000 :  4.24685151207447\n",
      "Average loss at step  750000 :  4.344156651258468\n",
      "Average loss at step  752000 :  4.284543124556541\n",
      "Average loss at step  754000 :  4.3046796183586125\n",
      "Average loss at step  756000 :  4.277213864684105\n",
      "Average loss at step  758000 :  4.36926970744133\n",
      "Average loss at step  760000 :  4.116950182676315\n",
      "Average loss at step  762000 :  4.39022463786602\n",
      "Average loss at step  764000 :  4.090014670431614\n",
      "Average loss at step  766000 :  4.036443462848664\n",
      "Average loss at step  768000 :  4.341598901152611\n",
      "Average loss at step  770000 :  4.374631242871285\n",
      "Average loss at step  772000 :  4.338165561318397\n",
      "Average loss at step  774000 :  4.311519423365593\n",
      "Average loss at step  776000 :  4.087437375307083\n",
      "Average loss at step  778000 :  3.929238930821419\n",
      "Average loss at step  780000 :  3.7575454580783845\n",
      "Average loss at step  782000 :  3.9750794496536255\n",
      "Average loss at step  784000 :  4.159959733843803\n",
      "Average loss at step  786000 :  4.342655353546142\n",
      "Average loss at step  788000 :  4.304315619468689\n",
      "Average loss at step  790000 :  4.322067241072655\n",
      "Average loss at step  792000 :  4.3299488886594775\n",
      "Average loss at step  794000 :  4.303471229553223\n",
      "Average loss at step  796000 :  4.300039766907692\n",
      "Average loss at step  798000 :  4.283561715245247\n",
      "Average loss at step  800000 :  4.270893115758896\n",
      "Average loss at step  802000 :  4.294340628623963\n",
      "Average loss at step  804000 :  4.3563053077459335\n",
      "Average loss at step  806000 :  4.354524579524994\n",
      "Average loss at step  808000 :  4.292233490943909\n",
      "Average loss at step  810000 :  4.181676788449288\n",
      "Average loss at step  812000 :  4.302115718603134\n",
      "Average loss at step  814000 :  4.299452010154724\n",
      "Average loss at step  816000 :  4.215015769124031\n",
      "Average loss at step  818000 :  4.267001602768898\n",
      "Average loss at step  820000 :  5.288468070268631\n",
      "Average loss at step  822000 :  4.3410480874776844\n",
      "Average loss at step  824000 :  4.05386025762558\n",
      "Average loss at step  826000 :  4.9500802861452105\n",
      "Average loss at step  828000 :  3.9677530263662337\n",
      "Average loss at step  830000 :  3.9877890042066575\n",
      "Average loss at step  832000 :  4.0357089732885365\n",
      "Average loss at step  834000 :  4.026220373153686\n",
      "Average loss at step  836000 :  3.9974334734678267\n",
      "Average loss at step  838000 :  4.199840140342713\n",
      "Average loss at step  840000 :  4.4555366740226745\n",
      "Average loss at step  842000 :  4.2409495008289815\n",
      "Average loss at step  844000 :  4.398424120664597\n",
      "Average loss at step  846000 :  4.370685411572456\n",
      "Average loss at step  848000 :  4.2965311199426655\n",
      "Average loss at step  850000 :  4.210469011187554\n",
      "Average loss at step  852000 :  4.224837976753712\n",
      "Average loss at step  854000 :  4.314348256230354\n",
      "Average loss at step  856000 :  4.204943155050278\n",
      "Average loss at step  858000 :  4.268453116655349\n",
      "Average loss at step  860000 :  4.248562032341957\n",
      "Average loss at step  862000 :  4.339310414910316\n",
      "Average loss at step  864000 :  4.374213210701942\n",
      "Average loss at step  866000 :  4.309705422878265\n",
      "Average loss at step  868000 :  4.4058921190500255\n",
      "Average loss at step  870000 :  4.332958591222763\n",
      "Average loss at step  872000 :  4.315142304897308\n",
      "Average loss at step  874000 :  4.274878316164017\n",
      "Average loss at step  876000 :  4.283106027007103\n",
      "Average loss at step  878000 :  4.236976651310921\n",
      "Average loss at step  880000 :  4.478093474686146\n",
      "Average loss at step  882000 :  4.315529961585998\n",
      "Average loss at step  884000 :  4.204822598576546\n",
      "Average loss at step  886000 :  4.174728330254554\n",
      "Average loss at step  888000 :  4.174836911559105\n",
      "Average loss at step  890000 :  4.295378961801529\n",
      "Average loss at step  892000 :  4.306018877744675\n",
      "Average loss at step  894000 :  4.302001410722733\n",
      "Average loss at step  896000 :  4.204505324602127\n",
      "Average loss at step  898000 :  4.226044187664986\n",
      "Average loss at step  900000 :  4.219015820860863\n",
      "Average loss at step  902000 :  4.384496807336808\n",
      "Average loss at step  904000 :  3.9992243607044218\n",
      "Average loss at step  906000 :  3.952310382604599\n",
      "Average loss at step  908000 :  4.374086872696877\n",
      "Average loss at step  910000 :  4.275371341347695\n",
      "Average loss at step  912000 :  4.037676180005073\n",
      "Average loss at step  914000 :  4.191338356971741\n",
      "Average loss at step  916000 :  4.3285915284156795\n",
      "Average loss at step  918000 :  4.291620365262031\n",
      "Average loss at step  920000 :  4.260528679132461\n",
      "Average loss at step  922000 :  4.231504930973053\n",
      "Average loss at step  924000 :  4.335511237382889\n",
      "Average loss at step  926000 :  4.3100107257366185\n",
      "Average loss at step  928000 :  4.287699677348137\n",
      "Average loss at step  930000 :  4.109908051490784\n",
      "Average loss at step  932000 :  4.2421630192995075\n",
      "Average loss at step  934000 :  4.257129726529121\n",
      "Average loss at step  936000 :  4.307516729593277\n",
      "Average loss at step  938000 :  4.4555452181100845\n",
      "Average loss at step  940000 :  4.285328846693039\n",
      "Average loss at step  942000 :  4.3065411297082905\n",
      "Average loss at step  944000 :  4.239814739584923\n",
      "Average loss at step  946000 :  4.2117801575660705\n",
      "Average loss at step  948000 :  4.139800058245659\n",
      "Average loss at step  950000 :  4.128535090446472\n",
      "Average loss at step  952000 :  4.169936985492706\n",
      "Average loss at step  954000 :  4.2754537087082864\n",
      "Average loss at step  956000 :  4.3277708811759945\n",
      "Average loss at step  958000 :  4.202390609443188\n",
      "Average loss at step  960000 :  4.320132297873497\n",
      "Average loss at step  962000 :  4.219053433775902\n",
      "Average loss at step  964000 :  4.364687489628792\n",
      "Average loss at step  966000 :  4.251635825395584\n",
      "Average loss at step  968000 :  4.284351989150047\n",
      "Average loss at step  970000 :  4.165743663549423\n",
      "Average loss at step  972000 :  4.338614490151405\n",
      "Average loss at step  974000 :  4.352269739031792\n",
      "Average loss at step  976000 :  4.29974292743206\n",
      "Average loss at step  978000 :  4.265009386062622\n",
      "Average loss at step  980000 :  4.242497073411942\n",
      "Average loss at step  982000 :  4.32966208434105\n",
      "Average loss at step  984000 :  4.293624529123306\n",
      "Average loss at step  986000 :  4.267765787959099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  988000 :  4.275804115891456\n",
      "Average loss at step  990000 :  4.287743666529655\n",
      "Average loss at step  992000 :  4.278466325879097\n",
      "Average loss at step  994000 :  4.200004235029221\n",
      "Average loss at step  996000 :  4.294155029892922\n",
      "Average loss at step  998000 :  4.2209870955348014\n",
      "Average loss at step  1000000 :  4.321140387654305\n",
      "Average loss at step  1002000 :  4.323150376677513\n",
      "Average loss at step  1004000 :  4.2862759510278705\n",
      "Average loss at step  1006000 :  4.27425387096405\n",
      "Average loss at step  1008000 :  4.262414734959602\n",
      "Average loss at step  1010000 :  4.264902447223664\n",
      "Average loss at step  1012000 :  4.233947203040123\n",
      "Average loss at step  1014000 :  4.161872918486595\n",
      "Average loss at step  1016000 :  4.242193931221962\n",
      "Average loss at step  1018000 :  4.220278121232987\n",
      "Average loss at step  1020000 :  4.165673514485359\n",
      "Average loss at step  1022000 :  4.23076765525341\n",
      "Average loss at step  1024000 :  4.205060137152672\n",
      "Average loss at step  1026000 :  4.0540943512320515\n",
      "Average loss at step  1028000 :  4.301561552882195\n",
      "Average loss at step  1030000 :  4.326264390468597\n",
      "Average loss at step  1032000 :  4.243826955199242\n",
      "Average loss at step  1034000 :  4.340014307975769\n",
      "Average loss at step  1036000 :  4.311269743323326\n",
      "Average loss at step  1038000 :  4.283404303431511\n",
      "Average loss at step  1040000 :  4.191079034686089\n",
      "Average loss at step  1042000 :  4.147005441904068\n",
      "Average loss at step  1044000 :  4.245136987566948\n",
      "Average loss at step  1046000 :  4.312700609683991\n",
      "Average loss at step  1048000 :  4.261271391749382\n",
      "Average loss at step  1050000 :  4.215329664707184\n",
      "Average loss at step  1052000 :  4.145358694553376\n",
      "Average loss at step  1054000 :  4.324942849457264\n",
      "Average loss at step  1056000 :  4.313578228116035\n",
      "Average loss at step  1058000 :  4.2985019274950025\n",
      "Average loss at step  1060000 :  4.270382477164269\n",
      "Average loss at step  1062000 :  4.26594616818428\n",
      "Average loss at step  1064000 :  4.18019299530983\n",
      "Average loss at step  1066000 :  4.337126762032509\n",
      "Average loss at step  1068000 :  4.205948046922684\n",
      "Average loss at step  1070000 :  4.278066330313683\n",
      "Average loss at step  1072000 :  3.7716900008916854\n",
      "Average loss at step  1074000 :  4.174918154239655\n",
      "Average loss at step  1076000 :  4.272741410136223\n",
      "Average loss at step  1078000 :  4.274438042879105\n",
      "Average loss at step  1080000 :  4.17862954723835\n",
      "Average loss at step  1082000 :  4.2871871137619015\n",
      "Average loss at step  1084000 :  4.251393523931504\n",
      "Average loss at step  1086000 :  4.117803233861923\n",
      "Average loss at step  1088000 :  4.326273829340935\n",
      "Average loss at step  1090000 :  4.227487020850182\n",
      "Average loss at step  1092000 :  4.21762717628479\n",
      "Average loss at step  1094000 :  4.170433152794838\n",
      "Average loss at step  1096000 :  4.200113236188889\n",
      "Average loss at step  1098000 :  4.1690026367902755\n",
      "Average loss at step  1100000 :  4.201713384509087\n",
      "Average loss at step  1102000 :  4.178781046628952\n",
      "Average loss at step  1104000 :  4.188514708995819\n",
      "Average loss at step  1106000 :  4.3599136449098586\n",
      "Average loss at step  1108000 :  4.2947975881099705\n",
      "Average loss at step  1110000 :  4.247726058840752\n",
      "Average loss at step  1112000 :  4.183265449762344\n",
      "Average loss at step  1114000 :  4.1876617827415465\n",
      "Average loss at step  1116000 :  4.167656697154045\n",
      "Average loss at step  1118000 :  4.124904361963272\n",
      "Average loss at step  1120000 :  4.13366468322277\n",
      "Average loss at step  1122000 :  4.021564362645149\n",
      "Average loss at step  1124000 :  4.354778170228005\n",
      "Average loss at step  1126000 :  4.251300087571144\n",
      "Average loss at step  1128000 :  4.252339993834496\n",
      "Average loss at step  1130000 :  4.2529292498826985\n",
      "Average loss at step  1132000 :  4.253361770987511\n",
      "Average loss at step  1134000 :  4.215029826998711\n",
      "Average loss at step  1136000 :  4.210209956645966\n",
      "Average loss at step  1138000 :  4.242524479031562\n",
      "Average loss at step  1140000 :  4.249765079140663\n",
      "Average loss at step  1142000 :  4.313885733366012\n",
      "Average loss at step  1144000 :  4.25901162993908\n",
      "Average loss at step  1146000 :  4.239991072416306\n",
      "Average loss at step  1148000 :  4.233886620879173\n",
      "Average loss at step  1150000 :  4.231319744706154\n",
      "Average loss at step  1152000 :  4.3523821903467175\n",
      "Average loss at step  1154000 :  4.2848855835199355\n",
      "Average loss at step  1156000 :  4.222951323032379\n",
      "Average loss at step  1158000 :  4.250757993578911\n",
      "Average loss at step  1160000 :  4.343978737950325\n",
      "Average loss at step  1162000 :  4.2920166420936585\n",
      "Average loss at step  1164000 :  4.288067407011986\n",
      "Average loss at step  1166000 :  4.165507279157638\n",
      "Average loss at step  1168000 :  3.739877304315567\n",
      "Average loss at step  1170000 :  4.378143771648407\n",
      "Average loss at step  1172000 :  4.2768733015060425\n",
      "Average loss at step  1174000 :  4.243215133517981\n",
      "Average loss at step  1176000 :  4.256869962215424\n",
      "Average loss at step  1178000 :  4.234178324222564\n",
      "Average loss at step  1180000 :  4.1497827785015104\n",
      "Average loss at step  1182000 :  4.322174467802048\n",
      "Average loss at step  1184000 :  4.717538149356842\n",
      "Average loss at step  1186000 :  4.287729120016098\n",
      "Average loss at step  1188000 :  4.279897260189056\n",
      "Average loss at step  1190000 :  4.105654245376587\n",
      "Average loss at step  1192000 :  4.114156267285347\n",
      "Average loss at step  1194000 :  4.3172391307353974\n",
      "Average loss at step  1196000 :  4.18402403819561\n",
      "Average loss at step  1198000 :  4.24085041153431\n",
      "Average loss at step  1200000 :  4.20628429222107\n",
      "Average loss at step  1202000 :  4.244239417076111\n",
      "Average loss at step  1204000 :  4.277087213635444\n",
      "Average loss at step  1206000 :  4.230184087991715\n",
      "Average loss at step  1208000 :  4.19791226541996\n",
      "Average loss at step  1210000 :  4.246301250457764\n",
      "Average loss at step  1212000 :  4.27395316696167\n",
      "Average loss at step  1214000 :  4.1052314388155935\n",
      "Average loss at step  1216000 :  4.511454791545868\n",
      "Average loss at step  1218000 :  4.332982953071594\n",
      "Average loss at step  1220000 :  4.313213576436043\n",
      "Average loss at step  1222000 :  4.265835992455482\n",
      "Average loss at step  1224000 :  4.281775705337524\n",
      "Average loss at step  1226000 :  4.2671580710411074\n",
      "Average loss at step  1228000 :  4.297416032791138\n",
      "Average loss at step  1230000 :  4.297773735046387\n",
      "Average loss at step  1232000 :  4.277178732275963\n",
      "Average loss at step  1234000 :  4.334877241849899\n",
      "Average loss at step  1236000 :  4.275326493144036\n",
      "Average loss at step  1238000 :  4.2396257057189946\n",
      "Average loss at step  1240000 :  4.159716994047165\n",
      "Average loss at step  1242000 :  4.153177718043327\n",
      "Average loss at step  1244000 :  4.144362636089325\n",
      "Average loss at step  1246000 :  4.121401270389557\n",
      "Average loss at step  1248000 :  4.10847424197197\n",
      "Average loss at step  1250000 :  4.129420656204224\n",
      "Average loss at step  1252000 :  4.104830698490143\n",
      "Average loss at step  1254000 :  4.096281417608261\n",
      "Average loss at step  1256000 :  4.132812017917633\n",
      "Average loss at step  1258000 :  4.087284895658493\n",
      "Average loss at step  1260000 :  4.103309931278229\n",
      "Average loss at step  1262000 :  4.1075826797485355\n",
      "Average loss at step  1264000 :  4.217072669506073\n",
      "Average loss at step  1266000 :  4.2932288408279415\n",
      "Average loss at step  1268000 :  4.015600122094154\n",
      "Average loss at step  1270000 :  4.010801653265953\n",
      "Average loss at step  1272000 :  4.055457718014717\n",
      "Average loss at step  1274000 :  4.222091069459915\n",
      "Average loss at step  1276000 :  4.323775240063667\n",
      "Average loss at step  1278000 :  4.248414070010186\n",
      "Average loss at step  1280000 :  4.12117320549488\n",
      "Average loss at step  1282000 :  4.131843538880348\n",
      "Average loss at step  1284000 :  4.137596082091331\n",
      "Average loss at step  1286000 :  4.206448047697544\n",
      "Average loss at step  1288000 :  4.0994941996335985\n",
      "Average loss at step  1290000 :  4.365362622618675\n",
      "Average loss at step  1292000 :  4.185194700956345\n",
      "Average loss at step  1294000 :  4.362800554513932\n",
      "Average loss at step  1296000 :  4.173111543178559\n",
      "Average loss at step  1298000 :  4.353567427039146\n",
      "Average loss at step  1300000 :  4.069327925682068\n",
      "Average loss at step  1302000 :  3.902722653388977\n",
      "Average loss at step  1304000 :  4.285813014864922\n",
      "Average loss at step  1306000 :  4.067420193791389\n",
      "Average loss at step  1308000 :  4.144877917051315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  1310000 :  4.0577375831604\n",
      "Average loss at step  1312000 :  3.9974496145248413\n",
      "Average loss at step  1314000 :  4.0703355870246885\n",
      "Average loss at step  1316000 :  4.21774648976326\n",
      "Average loss at step  1318000 :  4.162503222048283\n",
      "Average loss at step  1320000 :  4.1034983917474745\n",
      "Average loss at step  1322000 :  4.034387312412262\n",
      "Average loss at step  1324000 :  4.1784110455513\n",
      "Average loss at step  1326000 :  4.127821078419685\n",
      "Average loss at step  1328000 :  4.294067766070366\n",
      "Average loss at step  1330000 :  4.192228624820709\n",
      "Average loss at step  1332000 :  4.317530919551849\n",
      "Average loss at step  1334000 :  4.333945018410683\n",
      "Average loss at step  1336000 :  4.206474745094776\n",
      "Average loss at step  1338000 :  4.284981201648712\n",
      "Average loss at step  1340000 :  4.2339026602506635\n",
      "Average loss at step  1342000 :  4.176721128702163\n",
      "Average loss at step  1344000 :  4.142412091732025\n",
      "Average loss at step  1346000 :  4.139553427815438\n",
      "Average loss at step  1348000 :  4.026869518637657\n",
      "Average loss at step  1350000 :  4.249080736517906\n",
      "Average loss at step  1352000 :  4.176737508296966\n",
      "Average loss at step  1354000 :  4.092708013296127\n",
      "Average loss at step  1356000 :  4.362089132070541\n",
      "Average loss at step  1358000 :  4.284864571213722\n",
      "Average loss at step  1360000 :  4.247657295703888\n",
      "Average loss at step  1362000 :  4.21832176142931\n",
      "Average loss at step  1364000 :  4.0068701338768005\n",
      "Average loss at step  1366000 :  4.013521729826927\n",
      "Average loss at step  1368000 :  4.0004745806455615\n",
      "Average loss at step  1370000 :  3.9537414422631265\n",
      "Average loss at step  1372000 :  3.9782141968011855\n",
      "Average loss at step  1374000 :  4.063614836454391\n",
      "Average loss at step  1376000 :  4.180265713214874\n",
      "Average loss at step  1378000 :  4.241353473305702\n",
      "Average loss at step  1380000 :  4.315428272247314\n",
      "Average loss at step  1382000 :  4.275501253128052\n",
      "Average loss at step  1384000 :  4.297496722459793\n",
      "Average loss at step  1386000 :  4.22255869820714\n",
      "Average loss at step  1388000 :  4.133057236909866\n",
      "Average loss at step  1390000 :  4.259378825783729\n",
      "Average loss at step  1392000 :  4.161090878248214\n",
      "Average loss at step  1394000 :  4.066475511312484\n",
      "Average loss at step  1396000 :  6.3093250029087065\n",
      "Average loss at step  1398000 :  4.001067588090897\n",
      "Average loss at step  1400000 :  3.8370121972560884\n",
      "Average loss at step  1402000 :  4.042328277587891\n",
      "Average loss at step  1404000 :  4.37425197672844\n",
      "Average loss at step  1406000 :  4.247716126561165\n",
      "Average loss at step  1408000 :  4.2850419974327085\n",
      "Average loss at step  1410000 :  4.298469615221023\n",
      "Average loss at step  1412000 :  4.226865654230118\n",
      "Average loss at step  1414000 :  4.304484991431236\n",
      "Average loss at step  1416000 :  4.168545380830765\n",
      "Average loss at step  1418000 :  4.25041564822197\n",
      "Average loss at step  1420000 :  4.122079879164696\n",
      "Average loss at step  1422000 :  4.218710928559303\n",
      "Average loss at step  1424000 :  4.2203008892536165\n",
      "Average loss at step  1426000 :  4.061071797251701\n",
      "Average loss at step  1428000 :  4.279925926327706\n",
      "Average loss at step  1430000 :  4.341537588000298\n",
      "Average loss at step  1432000 :  4.247100687265396\n",
      "Average loss at step  1434000 :  3.503595943272114\n",
      "Average loss at step  1436000 :  4.344622847914696\n",
      "Average loss at step  1438000 :  4.244330929875374\n",
      "Average loss at step  1440000 :  4.218338522672653\n",
      "Average loss at step  1442000 :  4.159941783547401\n",
      "Average loss at step  1444000 :  4.187556505560875\n",
      "Average loss at step  1446000 :  4.225135848045349\n",
      "Average loss at step  1448000 :  4.282494694113732\n",
      "Average loss at step  1450000 :  4.225627314090729\n",
      "Average loss at step  1452000 :  4.200085710525513\n",
      "Average loss at step  1454000 :  4.198127662181855\n",
      "Average loss at step  1456000 :  4.123021872401237\n",
      "Average loss at step  1458000 :  4.219484080076217\n",
      "Average loss at step  1460000 :  4.12118564838171\n",
      "Average loss at step  1462000 :  4.283970313191414\n",
      "Average loss at step  1464000 :  4.174277270913124\n",
      "Average loss at step  1466000 :  4.254323136091232\n",
      "Average loss at step  1468000 :  4.236405941128731\n",
      "Average loss at step  1470000 :  4.270222734212876\n",
      "Average loss at step  1472000 :  4.332195651650429\n",
      "Average loss at step  1474000 :  4.19171328485012\n",
      "Average loss at step  1476000 :  4.3688507534265515\n",
      "Average loss at step  1478000 :  4.311056966066361\n",
      "Average loss at step  1480000 :  4.22170828807354\n",
      "Average loss at step  1482000 :  4.213973154425621\n",
      "Average loss at step  1484000 :  4.224307667851448\n",
      "Average loss at step  1486000 :  4.266851003646851\n",
      "Average loss at step  1488000 :  4.27156822502613\n",
      "Average loss at step  1490000 :  4.164075071811676\n",
      "Average loss at step  1492000 :  4.182436039805412\n",
      "Average loss at step  1494000 :  4.323043737530709\n",
      "Average loss at step  1496000 :  4.187079373121262\n",
      "Average loss at step  1498000 :  4.339231245279312\n",
      "Average loss at step  1500000 :  4.17515492272377\n",
      "Average loss at step  1502000 :  4.289681021094323\n",
      "Average loss at step  1504000 :  4.611447649240493\n",
      "Average loss at step  1506000 :  4.245882053732872\n",
      "Average loss at step  1508000 :  4.3254653449058535\n",
      "Average loss at step  1510000 :  4.3201111919879915\n",
      "Average loss at step  1512000 :  4.2958340884447095\n",
      "Average loss at step  1514000 :  4.280577141046524\n",
      "Average loss at step  1516000 :  4.173002893805504\n",
      "Average loss at step  1518000 :  4.309929464936256\n",
      "Average loss at step  1520000 :  4.231050849676132\n",
      "Average loss at step  1522000 :  4.2286297701597215\n",
      "Average loss at step  1524000 :  4.171292066216469\n",
      "Average loss at step  1526000 :  4.192546669602394\n",
      "Average loss at step  1528000 :  4.189062100410461\n",
      "Average loss at step  1530000 :  4.052293333530426\n",
      "Average loss at step  1532000 :  4.115833166480065\n",
      "Average loss at step  1534000 :  4.291311709403992\n",
      "Average loss at step  1536000 :  4.073844148039818\n",
      "Average loss at step  1538000 :  4.120110509634018\n",
      "Average loss at step  1540000 :  4.267307473659516\n",
      "Average loss at step  1542000 :  3.9822767300605775\n",
      "Average loss at step  1544000 :  4.022610559403896\n",
      "Average loss at step  1546000 :  4.174164209008217\n",
      "Average loss at step  1548000 :  4.255110122680664\n",
      "Average loss at step  1550000 :  4.261033326625824\n",
      "Average loss at step  1552000 :  4.1233257576823235\n",
      "Average loss at step  1554000 :  4.268970586061478\n",
      "Average loss at step  1556000 :  4.237964574694633\n",
      "Average loss at step  1558000 :  4.138630611658097\n",
      "Average loss at step  1560000 :  4.237401398539543\n",
      "Average loss at step  1562000 :  4.168698062300682\n",
      "Average loss at step  1564000 :  4.188655914902687\n",
      "Average loss at step  1566000 :  4.225359619379043\n",
      "Average loss at step  1568000 :  4.266221985816956\n",
      "Average loss at step  1570000 :  4.276519581973552\n",
      "Average loss at step  1572000 :  4.152047719120979\n",
      "Average loss at step  1574000 :  4.279117632031441\n",
      "Average loss at step  1576000 :  4.182909732460976\n",
      "Average loss at step  1578000 :  4.049383996665478\n",
      "Average loss at step  1580000 :  3.9467625523507595\n",
      "Average loss at step  1582000 :  3.935799673885107\n",
      "Average loss at step  1584000 :  3.889957355260849\n",
      "Average loss at step  1586000 :  3.9565866688489915\n",
      "Average loss at step  1588000 :  5.479322004050016\n",
      "Average loss at step  1590000 :  4.213100440502167\n",
      "Average loss at step  1592000 :  4.15768675327301\n",
      "Average loss at step  1594000 :  4.356457918167115\n",
      "Average loss at step  1596000 :  4.2448537815213205\n",
      "Average loss at step  1598000 :  4.323855531692505\n",
      "Average loss at step  1600000 :  4.226183326482773\n",
      "Average loss at step  1602000 :  4.315671766877174\n",
      "Average loss at step  1604000 :  4.210760564446449\n",
      "Average loss at step  1606000 :  4.17302821958065\n",
      "Average loss at step  1608000 :  4.118238239884376\n",
      "Average loss at step  1610000 :  4.082714050889015\n",
      "Average loss at step  1612000 :  4.1571385229825975\n",
      "Average loss at step  1614000 :  4.159571581602097\n",
      "Average loss at step  1616000 :  4.191974769592285\n",
      "Average loss at step  1618000 :  4.035024331331253\n",
      "Average loss at step  1620000 :  4.244629590630531\n",
      "Average loss at step  1622000 :  4.100398020267487\n",
      "Average loss at step  1624000 :  4.297106654524804\n",
      "Average loss at step  1626000 :  4.0217331087589265\n",
      "Average loss at step  1628000 :  4.05106616705656\n",
      "Average loss at step  1630000 :  4.199228279948235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  1632000 :  4.301184451341629\n",
      "Average loss at step  1634000 :  4.227746104478836\n",
      "Average loss at step  1636000 :  4.266204268693924\n",
      "Average loss at step  1638000 :  4.238674377441407\n",
      "Average loss at step  1640000 :  4.009734712362289\n",
      "Average loss at step  1642000 :  4.014914191007614\n",
      "Average loss at step  1644000 :  3.9156537655591963\n",
      "Average loss at step  1646000 :  3.879579005360603\n",
      "Average loss at step  1648000 :  4.301160969734192\n",
      "Average loss at step  1650000 :  4.134996850371361\n",
      "Average loss at step  1652000 :  4.292805868029594\n",
      "Average loss at step  1654000 :  4.233048303306103\n",
      "Average loss at step  1656000 :  4.2436115576028826\n",
      "Average loss at step  1658000 :  4.200628898262978\n",
      "Average loss at step  1660000 :  4.313096574664116\n",
      "Average loss at step  1662000 :  4.046994143128395\n",
      "Average loss at step  1664000 :  4.2752828749418255\n",
      "Average loss at step  1666000 :  4.058737500965595\n",
      "Average loss at step  1668000 :  3.8789762374162673\n",
      "Average loss at step  1670000 :  4.23987699174881\n",
      "Average loss at step  1672000 :  4.28728561091423\n",
      "Average loss at step  1674000 :  4.240413095831871\n",
      "Average loss at step  1676000 :  4.2436948626041415\n",
      "Average loss at step  1678000 :  4.007843448519707\n",
      "Average loss at step  1680000 :  3.841302492260933\n",
      "Average loss at step  1682000 :  3.6668270100951195\n",
      "Average loss at step  1684000 :  3.881258343935013\n",
      "Average loss at step  1686000 :  4.0234121966362\n",
      "Average loss at step  1688000 :  4.294790915131569\n",
      "Average loss at step  1690000 :  4.221336931347847\n",
      "Average loss at step  1692000 :  4.261415225028991\n",
      "Average loss at step  1694000 :  4.2650229028463365\n",
      "Average loss at step  1696000 :  4.269402650237083\n",
      "Average loss at step  1698000 :  4.245129569649697\n",
      "Average loss at step  1700000 :  4.24175705742836\n",
      "Average loss at step  1702000 :  4.2359674824476246\n",
      "Average loss at step  1704000 :  4.236077380180359\n",
      "Average loss at step  1706000 :  4.273773706197739\n",
      "Average loss at step  1708000 :  4.240404441833496\n",
      "Average loss at step  1710000 :  4.259618030786514\n",
      "Average loss at step  1712000 :  4.120575369596481\n",
      "Average loss at step  1714000 :  4.212661646842957\n",
      "Average loss at step  1716000 :  4.185590540885925\n",
      "Average loss at step  1718000 :  4.172799560666085\n",
      "Average loss at step  1720000 :  4.195761040210724\n",
      "Average loss at step  1722000 :  4.982443502902985\n",
      "Average loss at step  1724000 :  4.3507165979743005\n",
      "Average loss at step  1726000 :  4.068580376029015\n",
      "Average loss at step  1728000 :  4.106437965750694\n",
      "Average loss at step  1730000 :  3.8767285475730895\n",
      "Average loss at step  1732000 :  3.872356683969498\n",
      "Average loss at step  1734000 :  4.00347067463398\n",
      "Average loss at step  1736000 :  3.972702321648598\n",
      "Average loss at step  1738000 :  3.905723776578903\n",
      "Average loss at step  1740000 :  4.0294102210998535\n",
      "Average loss at step  1742000 :  4.380911306977272\n",
      "Average loss at step  1744000 :  4.143457539349795\n",
      "Average loss at step  1746000 :  4.3325069799423215\n",
      "Average loss at step  1748000 :  4.3075666990280155\n",
      "Average loss at step  1750000 :  4.226317320585251\n",
      "Average loss at step  1752000 :  4.154611413478851\n",
      "Average loss at step  1754000 :  4.124812675148249\n",
      "Average loss at step  1756000 :  4.1633299241065975\n",
      "Average loss at step  1758000 :  4.095206194996834\n",
      "Average loss at step  1760000 :  4.151203196525573\n",
      "Average loss at step  1762000 :  4.1511602092981335\n",
      "Average loss at step  1764000 :  4.2651698441505435\n",
      "Average loss at step  1766000 :  4.336017784357071\n",
      "Average loss at step  1768000 :  4.257331381678581\n",
      "Average loss at step  1770000 :  4.3503070793151855\n",
      "Average loss at step  1772000 :  4.273736719191074\n",
      "Average loss at step  1774000 :  4.259283736824989\n",
      "Average loss at step  1776000 :  4.236425719022751\n",
      "Average loss at step  1778000 :  4.211277335762977\n",
      "Average loss at step  1780000 :  4.21059592461586\n",
      "Average loss at step  1782000 :  4.324450163841248\n",
      "Average loss at step  1784000 :  4.2741559985876085\n",
      "Average loss at step  1786000 :  4.161624050498009\n",
      "Average loss at step  1788000 :  4.135902640223503\n",
      "Average loss at step  1790000 :  4.102017938137054\n",
      "Average loss at step  1792000 :  4.2235397808551784\n",
      "Average loss at step  1794000 :  4.204832800269127\n",
      "Average loss at step  1796000 :  4.290192908644676\n",
      "Average loss at step  1798000 :  4.163239947199822\n",
      "Average loss at step  1800000 :  4.160724618196488\n",
      "Average loss at step  1802000 :  4.157512397408485\n",
      "Average loss at step  1804000 :  4.276364102959633\n",
      "Average loss at step  1806000 :  3.9352236107587815\n",
      "Average loss at step  1808000 :  3.8820417767763136\n",
      "Average loss at step  1810000 :  4.24441893351078\n",
      "Average loss at step  1812000 :  4.20188131248951\n",
      "Average loss at step  1814000 :  4.053036203503608\n",
      "Average loss at step  1816000 :  3.984055691242218\n",
      "Average loss at step  1818000 :  4.304277114152908\n",
      "Average loss at step  1820000 :  4.187699157118797\n",
      "Average loss at step  1822000 :  4.239293322920799\n",
      "Average loss at step  1824000 :  4.180838596463204\n",
      "Average loss at step  1826000 :  4.26016020989418\n",
      "Average loss at step  1828000 :  4.271369054436684\n",
      "Average loss at step  1830000 :  4.213497754693031\n",
      "Average loss at step  1832000 :  4.084402531027794\n",
      "Average loss at step  1834000 :  4.13846472299099\n",
      "Average loss at step  1836000 :  4.218108343839646\n",
      "Average loss at step  1838000 :  4.209319050192833\n",
      "Average loss at step  1840000 :  4.412620017409325\n",
      "Average loss at step  1842000 :  4.22545918738842\n",
      "Average loss at step  1844000 :  4.289282686233521\n",
      "Average loss at step  1846000 :  4.200714967012406\n",
      "Average loss at step  1848000 :  4.1741266382932665\n",
      "Average loss at step  1850000 :  4.116845935463905\n",
      "Average loss at step  1852000 :  4.082132860422134\n",
      "Average loss at step  1854000 :  4.100626658439636\n",
      "Average loss at step  1856000 :  4.200049394845963\n",
      "Average loss at step  1858000 :  4.293645300865173\n",
      "Average loss at step  1860000 :  4.131964153766632\n",
      "Average loss at step  1862000 :  4.2493768551349635\n",
      "Average loss at step  1864000 :  4.175400766968727\n",
      "Average loss at step  1866000 :  4.301269033432007\n",
      "Average loss at step  1868000 :  4.222104764699936\n",
      "Average loss at step  1870000 :  4.219596703529358\n",
      "Average loss at step  1872000 :  4.119963519215584\n",
      "Average loss at step  1874000 :  4.261094038009643\n",
      "Average loss at step  1876000 :  4.307269148468971\n",
      "Average loss at step  1878000 :  4.260868843197823\n",
      "Average loss at step  1880000 :  4.2317186034917835\n",
      "Average loss at step  1882000 :  4.17824746954441\n",
      "Average loss at step  1884000 :  4.261958230018616\n",
      "Average loss at step  1886000 :  4.258771845459938\n",
      "Average loss at step  1888000 :  4.2339433543682095\n",
      "Average loss at step  1890000 :  4.242603823304177\n",
      "Average loss at step  1892000 :  4.229967216014862\n",
      "Average loss at step  1894000 :  4.221179813861847\n",
      "Average loss at step  1896000 :  4.160478588938713\n",
      "Average loss at step  1898000 :  4.232199502468109\n",
      "Average loss at step  1900000 :  4.185364290773869\n",
      "Average loss at step  1902000 :  4.274084408283233\n",
      "Average loss at step  1904000 :  4.295256521701813\n",
      "Average loss at step  1906000 :  4.254854205489159\n",
      "Average loss at step  1908000 :  4.237804132699966\n",
      "Average loss at step  1910000 :  4.213245980024338\n",
      "Average loss at step  1912000 :  4.2298099037408825\n",
      "Average loss at step  1914000 :  4.2067263058424\n",
      "Average loss at step  1916000 :  4.140587027430534\n",
      "Average loss at step  1918000 :  4.189048999905586\n",
      "Average loss at step  1920000 :  4.167528160691261\n",
      "Average loss at step  1922000 :  4.143516815900803\n",
      "Average loss at step  1924000 :  4.177967557430267\n",
      "Average loss at step  1926000 :  4.15020618736744\n",
      "Average loss at step  1928000 :  4.100678952634334\n",
      "Average loss at step  1930000 :  4.153887589037418\n",
      "Average loss at step  1932000 :  4.2798649915456775\n",
      "Average loss at step  1934000 :  4.1818315134048465\n",
      "Average loss at step  1936000 :  4.286140592813492\n",
      "Average loss at step  1938000 :  4.255489322543144\n",
      "Average loss at step  1940000 :  4.256395372033119\n",
      "Average loss at step  1942000 :  4.1889461325407025\n",
      "Average loss at step  1944000 :  4.122082569003105\n",
      "Average loss at step  1946000 :  4.163254971146584\n",
      "Average loss at step  1948000 :  4.274199201107026\n",
      "Average loss at step  1950000 :  4.25363684952259\n",
      "Average loss at step  1952000 :  4.17064381301403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  1954000 :  4.141573465108872\n",
      "Average loss at step  1956000 :  4.269866013884545\n",
      "Average loss at step  1958000 :  4.234514716565609\n",
      "Average loss at step  1960000 :  4.264219604611397\n",
      "Average loss at step  1962000 :  4.22438660633564\n",
      "Average loss at step  1964000 :  4.224756456613541\n",
      "Average loss at step  1966000 :  4.1729965893030165\n",
      "Average loss at step  1968000 :  4.273772522091866\n",
      "Average loss at step  1970000 :  4.161711098074913\n",
      "Average loss at step  1972000 :  4.221881257653236\n",
      "Average loss at step  1974000 :  3.7160263779163363\n",
      "Average loss at step  1976000 :  4.055638767421246\n",
      "Average loss at step  1978000 :  4.232180942893028\n",
      "Average loss at step  1980000 :  4.2462973269224165\n",
      "Average loss at step  1982000 :  4.091560093045235\n",
      "Average loss at step  1984000 :  4.24774438905716\n",
      "Average loss at step  1986000 :  4.212187486290932\n",
      "Average loss at step  1988000 :  4.097468347787857\n",
      "Average loss at step  1990000 :  4.265831126570702\n",
      "Average loss at step  1992000 :  4.198968621969223\n",
      "Average loss at step  1994000 :  4.177994316458702\n",
      "Average loss at step  1996000 :  4.14969947707653\n",
      "Average loss at step  1998000 :  4.149783315062523\n",
      "Average loss at step  2000000 :  4.132018235683441\n",
      "Average loss at step  2002000 :  4.170077872395516\n",
      "Average loss at step  2004000 :  4.131583156585694\n",
      "Average loss at step  2006000 :  4.139771016597748\n",
      "Average loss at step  2008000 :  4.290427472352982\n",
      "Average loss at step  2010000 :  4.272302457809448\n",
      "Average loss at step  2012000 :  4.221002086162567\n",
      "Average loss at step  2014000 :  4.170736378550529\n",
      "Average loss at step  2016000 :  4.106832640647888\n",
      "Average loss at step  2018000 :  4.113520730733871\n",
      "Average loss at step  2020000 :  4.083682707428932\n",
      "Average loss at step  2022000 :  4.097447806477547\n",
      "Average loss at step  2024000 :  3.9728248364925385\n",
      "Average loss at step  2026000 :  4.265475302815437\n",
      "Average loss at step  2028000 :  4.215464265346527\n",
      "Average loss at step  2030000 :  4.2047348985671995\n",
      "Average loss at step  2032000 :  4.228754228234291\n",
      "Average loss at step  2034000 :  4.204845525026322\n",
      "Average loss at step  2036000 :  4.191290400266647\n",
      "Average loss at step  2038000 :  4.178151999950409\n",
      "Average loss at step  2040000 :  4.196336505293846\n",
      "Average loss at step  2042000 :  4.214898714184761\n",
      "Average loss at step  2044000 :  4.258561405658722\n",
      "Average loss at step  2046000 :  4.239373900771141\n",
      "Average loss at step  2048000 :  4.208446073412895\n",
      "Average loss at step  2050000 :  4.204641211152077\n",
      "Average loss at step  2052000 :  4.20096488571167\n",
      "Average loss at step  2054000 :  4.3064734077453615\n",
      "Average loss at step  2056000 :  4.2499057222604755\n",
      "Average loss at step  2058000 :  4.2076348558664325\n",
      "Average loss at step  2060000 :  4.207505255103111\n",
      "Average loss at step  2062000 :  4.314404372215271\n",
      "Average loss at step  2064000 :  4.2671950380802155\n",
      "Average loss at step  2066000 :  4.2487517821788785\n",
      "Average loss at step  2068000 :  4.168884631991387\n",
      "Average loss at step  2070000 :  3.6177633752226828\n",
      "Average loss at step  2072000 :  4.349099628210068\n",
      "Average loss at step  2074000 :  4.243608583331108\n",
      "Average loss at step  2076000 :  4.2038473730087285\n",
      "Average loss at step  2078000 :  4.234137472271919\n",
      "Average loss at step  2080000 :  4.202986386299133\n",
      "Average loss at step  2082000 :  4.036262201547623\n",
      "Average loss at step  2084000 :  4.237049357295036\n",
      "Average loss at step  2086000 :  4.970295110821724\n",
      "Average loss at step  2088000 :  4.37106885945797\n",
      "Average loss at step  2090000 :  4.237331412971019\n",
      "Average loss at step  2092000 :  4.105900198936462\n",
      "Average loss at step  2094000 :  4.056034827589989\n",
      "Average loss at step  2096000 :  4.285347884297371\n",
      "Average loss at step  2098000 :  4.151677690863609\n",
      "Average loss at step  2100000 :  4.211745992183685\n",
      "Average loss at step  2102000 :  4.1695809835195545\n",
      "Average loss at step  2104000 :  4.183715301990509\n",
      "Average loss at step  2106000 :  4.262153693079949\n",
      "Average loss at step  2108000 :  4.200652203917503\n",
      "Average loss at step  2110000 :  4.165465083479881\n",
      "Average loss at step  2112000 :  4.181568826317787\n",
      "Average loss at step  2114000 :  4.25322609603405\n",
      "Average loss at step  2116000 :  4.092627403497696\n",
      "Average loss at step  2118000 :  4.281152016878128\n",
      "Average loss at step  2120000 :  4.27401095187664\n",
      "Average loss at step  2122000 :  4.266740734100342\n",
      "Average loss at step  2124000 :  4.266918410539627\n",
      "Average loss at step  2126000 :  4.2116343367099764\n",
      "Average loss at step  2128000 :  4.194369572997093\n",
      "Average loss at step  2130000 :  4.285972970604896\n",
      "Average loss at step  2132000 :  4.245464715600014\n",
      "Average loss at step  2134000 :  4.232535403847694\n",
      "Average loss at step  2136000 :  4.322660627961159\n",
      "Average loss at step  2138000 :  4.218881238222123\n",
      "Average loss at step  2140000 :  4.216384526729584\n",
      "Average loss at step  2142000 :  4.1340499638319015\n",
      "Average loss at step  2144000 :  4.1221285065412525\n",
      "Average loss at step  2146000 :  4.10946299290657\n",
      "Average loss at step  2148000 :  4.10136124765873\n",
      "Average loss at step  2150000 :  4.077781759381295\n",
      "Average loss at step  2152000 :  4.076861737251281\n",
      "Average loss at step  2154000 :  4.088821169376374\n",
      "Average loss at step  2156000 :  4.070060988307\n",
      "Average loss at step  2158000 :  4.094661559581756\n",
      "Average loss at step  2160000 :  4.090094882249832\n",
      "Average loss at step  2162000 :  4.072530007958412\n",
      "Average loss at step  2164000 :  4.077831532239914\n",
      "Average loss at step  2166000 :  4.144572006106377\n",
      "Average loss at step  2168000 :  4.290673335313797\n",
      "Average loss at step  2170000 :  3.981610010266304\n",
      "Average loss at step  2172000 :  3.9699822627305985\n",
      "Average loss at step  2174000 :  3.9514039137363435\n",
      "Average loss at step  2176000 :  4.16761971604824\n",
      "Average loss at step  2178000 :  4.299731669068336\n",
      "Average loss at step  2180000 :  4.208582452654839\n",
      "Average loss at step  2182000 :  4.1064197994470595\n",
      "Average loss at step  2184000 :  4.1228847311735155\n",
      "Average loss at step  2186000 :  4.078292832612991\n",
      "Average loss at step  2188000 :  4.161917827844619\n",
      "Average loss at step  2190000 :  4.075434280276299\n",
      "Average loss at step  2192000 :  4.302812016248703\n",
      "Average loss at step  2194000 :  4.1371390935182575\n",
      "Average loss at step  2196000 :  4.319165127396584\n",
      "Average loss at step  2198000 :  4.173070558786392\n",
      "Average loss at step  2200000 :  4.294221878051758\n",
      "Average loss at step  2202000 :  4.063974200606346\n",
      "Average loss at step  2204000 :  3.842788718819618\n",
      "Average loss at step  2206000 :  4.234154246926308\n",
      "Average loss at step  2208000 :  3.9442281131744386\n",
      "Average loss at step  2210000 :  4.141398558497429\n",
      "Average loss at step  2212000 :  4.021220665693283\n",
      "Average loss at step  2214000 :  3.9479202015399935\n",
      "Average loss at step  2216000 :  4.012006187319756\n",
      "Average loss at step  2218000 :  4.179666918039322\n",
      "Average loss at step  2220000 :  4.116115528643132\n",
      "Average loss at step  2222000 :  4.062071540236473\n",
      "Average loss at step  2224000 :  3.985002799510956\n",
      "Average loss at step  2226000 :  4.142889621734619\n",
      "Average loss at step  2228000 :  4.053199912071228\n",
      "Average loss at step  2230000 :  4.222459894418717\n",
      "Average loss at step  2232000 :  4.1324241275787355\n",
      "Average loss at step  2234000 :  4.303613379359246\n",
      "Average loss at step  2236000 :  4.317406995773315\n",
      "Average loss at step  2238000 :  4.136394757032394\n",
      "Average loss at step  2240000 :  4.293218505978584\n",
      "Average loss at step  2242000 :  4.200277130722999\n",
      "Average loss at step  2244000 :  4.158398265719414\n",
      "Average loss at step  2246000 :  4.1107538409233095\n",
      "Average loss at step  2248000 :  4.117073995828629\n",
      "Average loss at step  2250000 :  3.9848164826631547\n",
      "Average loss at step  2252000 :  4.21559543132782\n",
      "Average loss at step  2254000 :  4.138307478189469\n",
      "Average loss at step  2256000 :  4.034465345025063\n",
      "Average loss at step  2258000 :  4.324623273134232\n",
      "Average loss at step  2260000 :  4.275406570076942\n",
      "Average loss at step  2262000 :  4.212590318322182\n",
      "Average loss at step  2264000 :  4.214390146255493\n",
      "Average loss at step  2266000 :  3.945216486811638\n",
      "Average loss at step  2268000 :  3.9852986779212953\n",
      "Average loss at step  2270000 :  3.9344512120485304\n",
      "Average loss at step  2272000 :  3.9214563955664636\n",
      "Average loss at step  2274000 :  3.927898469924927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  2276000 :  4.024707376241684\n",
      "Average loss at step  2278000 :  4.107720814466476\n",
      "Average loss at step  2280000 :  4.220001764535904\n",
      "Average loss at step  2282000 :  4.260829266548157\n",
      "Average loss at step  2284000 :  4.25526229429245\n",
      "Average loss at step  2286000 :  4.271722667217254\n",
      "Average loss at step  2288000 :  4.19568721473217\n",
      "Average loss at step  2290000 :  4.090045099377632\n",
      "Average loss at step  2292000 :  4.214900269627571\n",
      "Average loss at step  2294000 :  4.136278287410736\n",
      "Average loss at step  2296000 :  4.028934328198433\n",
      "Average loss at step  2298000 :  5.507858932256698\n",
      "Average loss at step  2300000 :  3.989307016849518\n",
      "Average loss at step  2302000 :  3.828712734103203\n",
      "Average loss at step  2304000 :  3.910028406500816\n",
      "Average loss at step  2306000 :  4.343755675077438\n",
      "Average loss at step  2308000 :  4.231966336250305\n",
      "Average loss at step  2310000 :  4.228640057563782\n",
      "Average loss at step  2312000 :  4.268319671630859\n",
      "Average loss at step  2314000 :  4.195657071590424\n",
      "Average loss at step  2316000 :  4.27288274371624\n",
      "Average loss at step  2318000 :  4.1473555572032925\n",
      "Average loss at step  2320000 :  4.227139182567597\n",
      "Average loss at step  2322000 :  4.086303649425506\n",
      "Average loss at step  2324000 :  4.157943346261979\n",
      "Average loss at step  2326000 :  4.222006895661354\n",
      "Average loss at step  2328000 :  3.948236054062843\n",
      "Average loss at step  2330000 :  4.2624584017992015\n",
      "Average loss at step  2332000 :  4.332297609448433\n",
      "Average loss at step  2334000 :  4.234874064445496\n",
      "Average loss at step  2336000 :  3.52514704811573\n",
      "Average loss at step  2338000 :  4.1385654838085175\n",
      "Average loss at step  2340000 :  4.225050157546997\n",
      "Average loss at step  2342000 :  4.203342803835869\n",
      "Average loss at step  2344000 :  4.129724827408791\n",
      "Average loss at step  2346000 :  4.146676352083683\n",
      "Average loss at step  2348000 :  4.1440479955673215\n",
      "Average loss at step  2350000 :  4.2783549330234525\n",
      "Average loss at step  2352000 :  4.175762999296189\n",
      "Average loss at step  2354000 :  4.159723590493202\n",
      "Average loss at step  2356000 :  4.164347363710403\n",
      "Average loss at step  2358000 :  4.122998541116714\n",
      "Average loss at step  2360000 :  4.125648631453514\n",
      "Average loss at step  2362000 :  3.9861066888570784\n",
      "Average loss at step  2364000 :  4.2695510650873185\n",
      "Average loss at step  2366000 :  4.147215206623077\n",
      "Average loss at step  2368000 :  4.1868043862581255\n",
      "Average loss at step  2370000 :  4.259783434510231\n",
      "Average loss at step  2372000 :  4.215952216744423\n",
      "Average loss at step  2374000 :  4.284040795087814\n",
      "Average loss at step  2376000 :  4.182776343107223\n",
      "Average loss at step  2378000 :  4.275215146303177\n",
      "Average loss at step  2380000 :  4.292139302253723\n",
      "Average loss at step  2382000 :  4.254159124493599\n",
      "Average loss at step  2384000 :  4.203653776049614\n",
      "Average loss at step  2386000 :  4.198900181293488\n",
      "Average loss at step  2388000 :  4.24887440109253\n",
      "Average loss at step  2390000 :  4.247125192165375\n",
      "Average loss at step  2392000 :  4.1521017272472385\n",
      "Average loss at step  2394000 :  4.134048637390137\n",
      "Average loss at step  2396000 :  4.2884990694522855\n",
      "Average loss at step  2398000 :  4.158879791855812\n",
      "Average loss at step  2400000 :  4.3054765123128895\n",
      "Average loss at step  2402000 :  4.171449513196945\n",
      "Average loss at step  2404000 :  4.224630850315094\n",
      "Average loss at step  2406000 :  5.130441832780838\n",
      "Average loss at step  2408000 :  4.215613264203071\n",
      "Average loss at step  2410000 :  4.423776833057404\n",
      "Average loss at step  2412000 :  4.312301395177841\n",
      "Average loss at step  2414000 :  4.268884740352631\n",
      "Average loss at step  2416000 :  4.2247671650648115\n",
      "Average loss at step  2418000 :  4.13817706644535\n",
      "Average loss at step  2420000 :  4.291126647114754\n",
      "Average loss at step  2422000 :  4.225589860081673\n",
      "Average loss at step  2424000 :  4.207707722067833\n",
      "Average loss at step  2426000 :  4.130614980340004\n",
      "Average loss at step  2428000 :  4.185571958303451\n",
      "Average loss at step  2430000 :  4.151157527923584\n",
      "Average loss at step  2432000 :  4.052434982419014\n",
      "Average loss at step  2434000 :  4.083415724635124\n",
      "Average loss at step  2436000 :  4.238208951354027\n",
      "Average loss at step  2438000 :  4.068671111226082\n",
      "Average loss at step  2440000 :  4.109302567839623\n",
      "Average loss at step  2442000 :  4.252570210516453\n",
      "Average loss at step  2444000 :  3.9773419802188874\n",
      "Average loss at step  2446000 :  3.9530779302120207\n",
      "Average loss at step  2448000 :  4.1273288880586625\n",
      "Average loss at step  2450000 :  4.233088585019112\n",
      "Average loss at step  2452000 :  4.191501682043076\n",
      "Average loss at step  2454000 :  4.078678863227368\n",
      "Average loss at step  2456000 :  4.2366097379922865\n",
      "Average loss at step  2458000 :  4.247335820794105\n",
      "Average loss at step  2460000 :  4.11155688726902\n",
      "Average loss at step  2462000 :  4.187662051439285\n",
      "Average loss at step  2464000 :  4.16148642873764\n",
      "Average loss at step  2466000 :  4.1320178546905515\n",
      "Average loss at step  2468000 :  4.198772120475769\n",
      "Average loss at step  2470000 :  4.259377327203751\n",
      "Average loss at step  2472000 :  4.245062674880028\n",
      "Average loss at step  2474000 :  4.11167403793335\n",
      "Average loss at step  2476000 :  4.2965741082429885\n",
      "Average loss at step  2478000 :  4.192507961750031\n",
      "Average loss at step  2480000 :  3.974361846268177\n",
      "Average loss at step  2482000 :  3.877700049996376\n",
      "Average loss at step  2484000 :  3.899965133666992\n",
      "Average loss at step  2486000 :  3.8546911804676056\n",
      "Average loss at step  2488000 :  3.8536146647930147\n",
      "Average loss at step  2490000 :  5.450563940048218\n",
      "Average loss at step  2492000 :  4.185193678021431\n",
      "Average loss at step  2494000 :  4.177071192979812\n",
      "Average loss at step  2496000 :  4.587279456138611\n",
      "Average loss at step  2498000 :  4.278764403253794\n",
      "Average loss at step  2500000 :  4.284243242621422\n",
      "Average loss at step  2502000 :  4.191110547184945\n",
      "Average loss at step  2504000 :  4.278342811107636\n",
      "Average loss at step  2506000 :  4.204690517783165\n",
      "Average loss at step  2508000 :  4.148662852525711\n",
      "Average loss at step  2510000 :  4.0825119754076\n",
      "Average loss at step  2512000 :  4.054800925135613\n",
      "Average loss at step  2514000 :  4.115613592982292\n",
      "Average loss at step  2516000 :  4.126683764100075\n",
      "Average loss at step  2518000 :  4.171390424370766\n",
      "Average loss at step  2520000 :  3.9549404236078263\n",
      "Average loss at step  2522000 :  4.238202594637871\n",
      "Average loss at step  2524000 :  4.01606105837226\n",
      "Average loss at step  2526000 :  4.268309279918671\n",
      "Average loss at step  2528000 :  4.233010925471783\n",
      "Average loss at step  2530000 :  3.945235601425171\n",
      "Average loss at step  2532000 :  4.1744005333185195\n",
      "Average loss at step  2534000 :  4.249536135315895\n",
      "Average loss at step  2536000 :  4.216482572555542\n",
      "Average loss at step  2538000 :  4.232803509831428\n",
      "Average loss at step  2540000 :  4.226249529004097\n",
      "Average loss at step  2542000 :  3.9924687675237656\n",
      "Average loss at step  2544000 :  4.0280988156795505\n",
      "Average loss at step  2546000 :  3.846669640779495\n",
      "Average loss at step  2548000 :  3.7486947053670883\n",
      "Average loss at step  2550000 :  4.308653485894204\n",
      "Average loss at step  2552000 :  4.110220510005951\n",
      "Average loss at step  2554000 :  4.267095379233361\n",
      "Average loss at step  2556000 :  4.2086145230531695\n",
      "Average loss at step  2558000 :  4.211081755489111\n",
      "Average loss at step  2560000 :  4.215062348842621\n",
      "Average loss at step  2562000 :  4.312050164818764\n",
      "Average loss at step  2564000 :  4.050815610170364\n",
      "Average loss at step  2566000 :  4.219807343721389\n",
      "Average loss at step  2568000 :  4.070166422367096\n",
      "Average loss at step  2570000 :  3.7964591833353043\n",
      "Average loss at step  2572000 :  4.264981836557388\n",
      "Average loss at step  2574000 :  4.252838185429573\n",
      "Average loss at step  2576000 :  4.208197703301907\n",
      "Average loss at step  2578000 :  4.232535428881645\n",
      "Average loss at step  2580000 :  3.997349224925041\n",
      "Average loss at step  2582000 :  3.837866961836815\n",
      "Average loss at step  2584000 :  3.632639786362648\n",
      "Average loss at step  2586000 :  3.822267089366913\n",
      "Average loss at step  2588000 :  3.9236856155395508\n",
      "Average loss at step  2590000 :  4.285362773180008\n",
      "Average loss at step  2592000 :  4.171280302166939\n",
      "Average loss at step  2594000 :  4.257012321352959\n",
      "Average loss at step  2596000 :  4.231685910105705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  2598000 :  4.254494772791863\n",
      "Average loss at step  2600000 :  4.233715772509575\n",
      "Average loss at step  2602000 :  4.234292388796806\n",
      "Average loss at step  2604000 :  4.224221024990082\n",
      "Average loss at step  2606000 :  4.212245034217834\n",
      "Average loss at step  2608000 :  4.265938498735428\n",
      "Average loss at step  2610000 :  4.204441958546639\n",
      "Average loss at step  2612000 :  4.20846888422966\n",
      "Average loss at step  2614000 :  4.121023378372192\n",
      "Average loss at step  2616000 :  4.179458324670792\n",
      "Average loss at step  2618000 :  4.1643834614753725\n",
      "Average loss at step  2620000 :  4.16285225725174\n",
      "Average loss at step  2622000 :  4.15439623439312\n",
      "Average loss at step  2624000 :  4.792579478263855\n",
      "Average loss at step  2626000 :  4.191925322771072\n",
      "Average loss at step  2628000 :  4.081801466226578\n",
      "Average loss at step  2630000 :  4.103252068161964\n",
      "Average loss at step  2632000 :  3.8457819349765776\n",
      "Average loss at step  2634000 :  3.8510222930908204\n",
      "Average loss at step  2636000 :  3.9642721339464186\n",
      "Average loss at step  2638000 :  3.9529983069896697\n",
      "Average loss at step  2640000 :  3.840791157722473\n",
      "Average loss at step  2642000 :  3.932551046013832\n",
      "Average loss at step  2644000 :  4.357624735832214\n",
      "Average loss at step  2646000 :  4.222670070588589\n",
      "Average loss at step  2648000 :  4.201152790248394\n",
      "Average loss at step  2650000 :  4.304783408761025\n",
      "Average loss at step  2652000 :  4.200202869534492\n",
      "Average loss at step  2654000 :  4.133135825753212\n",
      "Average loss at step  2656000 :  4.143733752459288\n",
      "Average loss at step  2658000 :  4.193501567184925\n",
      "Average loss at step  2660000 :  4.063247477531433\n",
      "Average loss at step  2662000 :  4.111372186899185\n",
      "Average loss at step  2664000 :  4.127933148980141\n",
      "Average loss at step  2666000 :  4.191494545221329\n",
      "Average loss at step  2668000 :  4.323425535798073\n",
      "Average loss at step  2670000 :  4.23654937183857\n",
      "Average loss at step  2672000 :  4.318770349025726\n",
      "Average loss at step  2674000 :  4.263091746211052\n",
      "Average loss at step  2676000 :  4.234866251468659\n",
      "Average loss at step  2678000 :  4.214225692451\n",
      "Average loss at step  2680000 :  4.180937285661697\n",
      "Average loss at step  2682000 :  4.187594183802605\n",
      "Average loss at step  2684000 :  4.591682057023048\n",
      "Average loss at step  2686000 :  4.27117698776722\n",
      "Average loss at step  2688000 :  4.158013371109963\n",
      "Average loss at step  2690000 :  4.123252075314522\n",
      "Average loss at step  2692000 :  4.0877685711383815\n",
      "Average loss at step  2694000 :  4.16484884083271\n",
      "Average loss at step  2696000 :  4.177117959976196\n",
      "Average loss at step  2698000 :  4.293190842032432\n",
      "Average loss at step  2700000 :  4.134906911492347\n",
      "Average loss at step  2702000 :  4.1583351790905\n",
      "Average loss at step  2704000 :  4.12385140299797\n",
      "Average loss at step  2706000 :  4.249196454286575\n",
      "Average loss at step  2708000 :  3.9362244670987128\n",
      "Average loss at step  2710000 :  3.845349111676216\n",
      "Average loss at step  2712000 :  4.175184701442719\n",
      "Average loss at step  2714000 :  4.194494394540786\n",
      "Average loss at step  2716000 :  4.071432324051857\n",
      "Average loss at step  2718000 :  3.8396501212120056\n",
      "Average loss at step  2720000 :  4.316164426922798\n",
      "Average loss at step  2722000 :  4.139358953118324\n",
      "Average loss at step  2724000 :  4.238695687174797\n",
      "Average loss at step  2726000 :  4.148038378000259\n",
      "Average loss at step  2728000 :  4.24575057888031\n",
      "Average loss at step  2730000 :  4.224486291527748\n",
      "Average loss at step  2732000 :  4.191271818637848\n",
      "Average loss at step  2734000 :  4.070348938047886\n",
      "Average loss at step  2736000 :  4.07177363049984\n",
      "Average loss at step  2738000 :  4.213567116856575\n",
      "Average loss at step  2740000 :  4.170802472829819\n",
      "Average loss at step  2742000 :  4.3688112069368366\n",
      "Average loss at step  2744000 :  4.230564425110817\n",
      "Average loss at step  2746000 :  4.248054086208343\n",
      "Average loss at step  2748000 :  4.190809283852577\n",
      "Average loss at step  2750000 :  4.1555217664241795\n",
      "Average loss at step  2752000 :  4.109243002295494\n",
      "Average loss at step  2754000 :  4.053600937724114\n",
      "Average loss at step  2756000 :  4.102207486391068\n",
      "Average loss at step  2758000 :  4.161140336215496\n",
      "Average loss at step  2760000 :  4.284188716053963\n",
      "Average loss at step  2762000 :  4.1229388448596005\n",
      "Average loss at step  2764000 :  4.200965497851372\n",
      "Average loss at step  2766000 :  4.176751502275467\n",
      "Average loss at step  2768000 :  4.257912521123886\n",
      "Average loss at step  2770000 :  4.215169305562973\n",
      "Average loss at step  2772000 :  4.1920930428504946\n",
      "Average loss at step  2774000 :  4.144998163580895\n",
      "Average loss at step  2776000 :  4.189404647350312\n",
      "Average loss at step  2778000 :  4.285798398137093\n",
      "Average loss at step  2780000 :  4.2619306491613385\n",
      "Average loss at step  2782000 :  4.214337079763412\n",
      "Average loss at step  2784000 :  4.189062041580677\n",
      "Average loss at step  2786000 :  4.216490038871765\n",
      "Average loss at step  2788000 :  4.259964033246041\n",
      "Average loss at step  2790000 :  4.189725871920586\n",
      "Average loss at step  2792000 :  4.235943076252937\n",
      "Average loss at step  2794000 :  4.223882702231407\n",
      "Average loss at step  2796000 :  4.159860347509384\n",
      "Average loss at step  2798000 :  4.158756944417953\n",
      "Average loss at step  2800000 :  4.190000384569168\n",
      "Average loss at step  2802000 :  4.169888392627239\n",
      "Average loss at step  2804000 :  4.234529930114746\n",
      "Average loss at step  2806000 :  4.285381838917732\n",
      "Average loss at step  2808000 :  4.230408254623413\n",
      "Average loss at step  2810000 :  4.2260089381933215\n",
      "Average loss at step  2812000 :  4.179472421169281\n",
      "Average loss at step  2814000 :  4.240287352204323\n",
      "Average loss at step  2816000 :  4.174641915559769\n",
      "Average loss at step  2818000 :  4.128963276982307\n",
      "Average loss at step  2820000 :  4.158975252270698\n",
      "Average loss at step  2822000 :  4.157599228978157\n",
      "Average loss at step  2824000 :  4.123205224275589\n",
      "Average loss at step  2826000 :  4.1597531795501705\n",
      "Average loss at step  2828000 :  4.14018095767498\n",
      "Average loss at step  2830000 :  4.189546376228333\n",
      "Average loss at step  2832000 :  4.035558027267456\n",
      "Average loss at step  2834000 :  4.269864841341972\n",
      "Average loss at step  2836000 :  4.152212127447128\n",
      "Average loss at step  2838000 :  4.299273352980614\n",
      "Average loss at step  2840000 :  4.213014573693275\n",
      "Average loss at step  2842000 :  4.235208827137947\n",
      "Average loss at step  2844000 :  4.1792821578979495\n",
      "Average loss at step  2846000 :  4.107179587841034\n",
      "Average loss at step  2848000 :  4.133424456357956\n",
      "Average loss at step  2850000 :  4.2590942150354385\n",
      "Average loss at step  2852000 :  4.245682564854622\n",
      "Average loss at step  2854000 :  4.136447885513306\n",
      "Average loss at step  2856000 :  4.13830914914608\n",
      "Average loss at step  2858000 :  4.2295459568500515\n",
      "Average loss at step  2860000 :  4.206088975697756\n",
      "Average loss at step  2862000 :  4.242132474780083\n",
      "Average loss at step  2864000 :  4.210417932629586\n",
      "Average loss at step  2866000 :  4.231816169500351\n",
      "Average loss at step  2868000 :  4.177035822272301\n",
      "Average loss at step  2870000 :  4.228236962556839\n",
      "Average loss at step  2872000 :  4.155865179181099\n",
      "Average loss at step  2874000 :  4.199358810067177\n",
      "Average loss at step  2876000 :  3.739753517687321\n",
      "Average loss at step  2878000 :  3.941085707068443\n",
      "Average loss at step  2880000 :  4.234344825744629\n",
      "Average loss at step  2882000 :  4.22147360253334\n",
      "Average loss at step  2884000 :  4.054682039499283\n",
      "Average loss at step  2886000 :  4.237079716801643\n",
      "Average loss at step  2888000 :  4.233156050086022\n",
      "Average loss at step  2890000 :  4.088982826590538\n",
      "Average loss at step  2892000 :  4.222155140042305\n",
      "Average loss at step  2894000 :  4.203667140960693\n",
      "Average loss at step  2896000 :  4.160220028042794\n",
      "Average loss at step  2898000 :  4.138616997599602\n",
      "Average loss at step  2900000 :  4.131025326251984\n",
      "Average loss at step  2902000 :  4.1049063556194305\n",
      "Average loss at step  2904000 :  4.157404125332833\n",
      "Average loss at step  2906000 :  4.115396393537521\n",
      "Average loss at step  2908000 :  4.129309478521347\n",
      "Average loss at step  2910000 :  4.215960947394371\n",
      "Average loss at step  2912000 :  4.287525671601295\n",
      "Average loss at step  2914000 :  4.206031940698623\n",
      "Average loss at step  2916000 :  4.150663496017456\n",
      "Average loss at step  2918000 :  4.100296692371368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  2920000 :  4.075579886317253\n",
      "Average loss at step  2922000 :  4.09094037604332\n",
      "Average loss at step  2924000 :  4.0748278311491015\n",
      "Average loss at step  2926000 :  3.965550338983536\n",
      "Average loss at step  2928000 :  4.188074489474297\n",
      "Average loss at step  2930000 :  4.224870565772057\n",
      "Average loss at step  2932000 :  4.190447001695633\n",
      "Average loss at step  2934000 :  4.221200376987457\n",
      "Average loss at step  2936000 :  4.2016355874538425\n",
      "Average loss at step  2938000 :  4.169414108633995\n",
      "Average loss at step  2940000 :  4.1800862069129945\n",
      "Average loss at step  2942000 :  4.199102093696594\n",
      "Average loss at step  2944000 :  4.196168918609619\n",
      "Average loss at step  2946000 :  4.226550813436508\n",
      "Average loss at step  2948000 :  4.224778766036033\n",
      "Average loss at step  2950000 :  4.194924988627434\n",
      "Average loss at step  2952000 :  4.196277834415436\n",
      "Average loss at step  2954000 :  4.172740723848343\n",
      "Average loss at step  2956000 :  4.285719827413559\n",
      "Average loss at step  2958000 :  4.254771224737167\n",
      "Average loss at step  2960000 :  4.19215857052803\n",
      "Average loss at step  2962000 :  4.199382503986358\n",
      "Average loss at step  2964000 :  4.283777703166008\n",
      "Average loss at step  2966000 :  4.254101109743118\n",
      "Average loss at step  2968000 :  4.221067313909531\n",
      "Average loss at step  2970000 :  4.168820083618164\n",
      "Average loss at step  2972000 :  3.7500670465528967\n",
      "Average loss at step  2974000 :  4.105417682588101\n",
      "Average loss at step  2976000 :  4.2627740428447725\n",
      "Average loss at step  2978000 :  4.197644401758909\n",
      "Average loss at step  2980000 :  4.22507916033268\n",
      "Average loss at step  2982000 :  4.188028527975082\n",
      "Average loss at step  2984000 :  3.9925401087999344\n",
      "Average loss at step  2986000 :  4.260501055121422\n",
      "Average loss at step  2988000 :  5.712129943281412\n",
      "Average loss at step  2990000 :  4.383821724772453\n",
      "Average loss at step  2992000 :  4.387110396623611\n",
      "Average loss at step  2994000 :  4.272737123250962\n",
      "Average loss at step  2996000 :  4.10597159421444\n",
      "Average loss at step  2998000 :  4.3167549011707305\n",
      "Average loss at step  3000000 :  4.255038763761521\n",
      "Average loss at step  3002000 :  4.217874196052551\n",
      "Average loss at step  3004000 :  4.157611180663109\n",
      "Average loss at step  3006000 :  4.142445521831513\n",
      "Average loss at step  3008000 :  4.274887974858284\n",
      "Average loss at step  3010000 :  4.18714277613163\n",
      "Average loss at step  3012000 :  4.187783811330795\n",
      "Average loss at step  3014000 :  4.182759448885918\n",
      "Average loss at step  3016000 :  4.224438456654549\n",
      "Average loss at step  3018000 :  4.112902035474777\n",
      "Average loss at step  3020000 :  4.205255149126053\n",
      "Average loss at step  3022000 :  4.277446588635445\n",
      "Average loss at step  3024000 :  4.261675888895988\n",
      "Average loss at step  3026000 :  4.24725929927826\n",
      "Average loss at step  3028000 :  4.171959412574768\n",
      "Average loss at step  3030000 :  4.234467111945152\n",
      "Average loss at step  3032000 :  4.237136659979821\n",
      "Average loss at step  3034000 :  4.233027529478073\n",
      "Average loss at step  3036000 :  4.235929992675781\n",
      "Average loss at step  3038000 :  4.289822127461433\n",
      "Average loss at step  3040000 :  4.239819725990295\n",
      "Average loss at step  3042000 :  4.201175449132919\n",
      "Average loss at step  3044000 :  4.137658465862274\n",
      "Average loss at step  3046000 :  4.1078379335403445\n",
      "Average loss at step  3048000 :  4.099431860685349\n",
      "Average loss at step  3050000 :  4.087446313619614\n",
      "Average loss at step  3052000 :  4.067695384502411\n",
      "Average loss at step  3054000 :  4.08918198466301\n",
      "Average loss at step  3056000 :  4.0694952294826505\n",
      "Average loss at step  3058000 :  4.059593803167343\n",
      "Average loss at step  3060000 :  4.058605585932732\n",
      "Average loss at step  3062000 :  4.077678376197815\n",
      "Average loss at step  3064000 :  4.0590859512090685\n",
      "Average loss at step  3066000 :  4.05593349635601\n",
      "Average loss at step  3068000 :  4.088495156049729\n",
      "Average loss at step  3070000 :  4.311921111941338\n",
      "Average loss at step  3072000 :  3.986505455493927\n",
      "Average loss at step  3074000 :  3.944991358160973\n",
      "Average loss at step  3076000 :  3.917312411785126\n",
      "Average loss at step  3078000 :  4.122886604130268\n",
      "Average loss at step  3080000 :  4.269390480041504\n",
      "Average loss at step  3082000 :  4.208458225727082\n",
      "Average loss at step  3084000 :  4.103283920288086\n",
      "Average loss at step  3086000 :  4.1224872500896454\n",
      "Average loss at step  3088000 :  4.050604474544525\n",
      "Average loss at step  3090000 :  4.159520823717117\n",
      "Average loss at step  3092000 :  4.068032575845718\n",
      "Average loss at step  3094000 :  4.257641743183136\n",
      "Average loss at step  3096000 :  4.1058669754266734\n",
      "Average loss at step  3098000 :  4.324285027623176\n",
      "Average loss at step  3100000 :  4.169597685337067\n",
      "Average loss at step  3102000 :  4.227963014125824\n",
      "Average loss at step  3104000 :  4.103675608038902\n",
      "Average loss at step  3106000 :  3.827136982798576\n",
      "Average loss at step  3108000 :  4.152659848213196\n",
      "Average loss at step  3110000 :  3.928791301012039\n",
      "Average loss at step  3112000 :  4.1112899454832075\n",
      "Average loss at step  3114000 :  4.015611961007118\n",
      "Average loss at step  3116000 :  3.9438913227319716\n",
      "Average loss at step  3118000 :  3.9779956777095795\n",
      "Average loss at step  3120000 :  4.153490711927414\n",
      "Average loss at step  3122000 :  4.105646753966808\n",
      "Average loss at step  3124000 :  4.038336403250694\n",
      "Average loss at step  3126000 :  3.975997609376907\n",
      "Average loss at step  3128000 :  4.095154734134674\n",
      "Average loss at step  3130000 :  4.044487182497978\n",
      "Average loss at step  3132000 :  4.1753183287382125\n",
      "Average loss at step  3134000 :  4.093747873902321\n",
      "Average loss at step  3136000 :  4.287564310669899\n",
      "Average loss at step  3138000 :  4.2930337768793105\n",
      "Average loss at step  3140000 :  4.144992797791958\n",
      "Average loss at step  3142000 :  4.244072785139084\n",
      "Average loss at step  3144000 :  4.171422383785248\n",
      "Average loss at step  3146000 :  4.162756147027015\n",
      "Average loss at step  3148000 :  4.088339477181434\n",
      "Average loss at step  3150000 :  4.129372383952141\n",
      "Average loss at step  3152000 :  3.956908853292465\n",
      "Average loss at step  3154000 :  4.179339749455452\n",
      "Average loss at step  3156000 :  4.116416502594948\n",
      "Average loss at step  3158000 :  3.983350336611271\n",
      "Average loss at step  3160000 :  4.30886162519455\n",
      "Average loss at step  3162000 :  4.285526035666466\n",
      "Average loss at step  3164000 :  4.194369959235192\n",
      "Average loss at step  3166000 :  4.208756173372269\n",
      "Average loss at step  3168000 :  3.924772900402546\n",
      "Average loss at step  3170000 :  3.9703105977773667\n",
      "Average loss at step  3172000 :  3.9182854133844374\n",
      "Average loss at step  3174000 :  3.900919710636139\n",
      "Average loss at step  3176000 :  3.912507023692131\n",
      "Average loss at step  3178000 :  3.9938510662317275\n",
      "Average loss at step  3180000 :  4.083467880308628\n",
      "Average loss at step  3182000 :  4.210209567904473\n",
      "Average loss at step  3184000 :  4.228855277180672\n",
      "Average loss at step  3186000 :  4.260803775191307\n",
      "Average loss at step  3188000 :  4.2593940985202785\n",
      "Average loss at step  3190000 :  4.205801513552665\n",
      "Average loss at step  3192000 :  4.057237645149231\n",
      "Average loss at step  3194000 :  4.191249873161316\n",
      "Average loss at step  3196000 :  4.1377967050075535\n",
      "Average loss at step  3198000 :  4.03082424902916\n",
      "Average loss at step  3200000 :  5.20960870295763\n",
      "Average loss at step  3202000 :  3.94563378071785\n",
      "Average loss at step  3204000 :  3.8735328978300094\n",
      "Average loss at step  3206000 :  3.8549083914756777\n",
      "Average loss at step  3208000 :  4.388356080293655\n",
      "Average loss at step  3210000 :  4.228007129192353\n",
      "Average loss at step  3212000 :  4.2333411501646045\n",
      "Average loss at step  3214000 :  4.222042400360108\n",
      "Average loss at step  3216000 :  4.1899533598423\n",
      "Average loss at step  3218000 :  4.23699278819561\n",
      "Average loss at step  3220000 :  4.147015437245369\n",
      "Average loss at step  3222000 :  4.205429358959198\n",
      "Average loss at step  3224000 :  4.046394092917442\n",
      "Average loss at step  3226000 :  4.146856159687042\n",
      "Average loss at step  3228000 :  4.2140010179281235\n",
      "Average loss at step  3230000 :  3.929086322903633\n",
      "Average loss at step  3232000 :  4.232847102046013\n",
      "Average loss at step  3234000 :  4.317504912734032\n",
      "Average loss at step  3236000 :  4.2517120201587675\n",
      "Average loss at step  3238000 :  3.601900317966938\n",
      "Average loss at step  3240000 :  3.989917790055275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  3242000 :  4.2056049457788465\n",
      "Average loss at step  3244000 :  4.20387511754036\n",
      "Average loss at step  3246000 :  4.110033952832222\n",
      "Average loss at step  3248000 :  4.10469421505928\n",
      "Average loss at step  3250000 :  4.112409308552742\n",
      "Average loss at step  3252000 :  4.280023613333702\n",
      "Average loss at step  3254000 :  4.191207775712013\n",
      "Average loss at step  3256000 :  4.1885323792696\n",
      "Average loss at step  3258000 :  4.170450583100319\n",
      "Average loss at step  3260000 :  4.125385766267777\n",
      "Average loss at step  3262000 :  4.152055392980576\n",
      "Average loss at step  3264000 :  3.8471131038069726\n",
      "Average loss at step  3266000 :  4.279925719022751\n",
      "Average loss at step  3268000 :  4.132791253447532\n",
      "Average loss at step  3270000 :  4.1727111964225765\n",
      "Average loss at step  3272000 :  4.236705763697624\n",
      "Average loss at step  3274000 :  4.207209437131882\n",
      "Average loss at step  3276000 :  4.2803676611185075\n",
      "Average loss at step  3278000 :  4.1847105119228365\n",
      "Average loss at step  3280000 :  4.202066350221634\n",
      "Average loss at step  3282000 :  4.291479553222656\n",
      "Average loss at step  3284000 :  4.2143662457466124\n",
      "Average loss at step  3286000 :  4.190274098038674\n",
      "Average loss at step  3288000 :  4.191261128902435\n",
      "Average loss at step  3290000 :  4.205411235928535\n",
      "Average loss at step  3292000 :  4.249046743512154\n",
      "Average loss at step  3294000 :  4.137824683785438\n",
      "Average loss at step  3296000 :  4.115882090330124\n",
      "Average loss at step  3298000 :  4.274394093155861\n",
      "Average loss at step  3300000 :  4.118615812897682\n",
      "Average loss at step  3302000 :  4.29869313120842\n",
      "Average loss at step  3304000 :  4.183088821172714\n",
      "Average loss at step  3306000 :  4.189804800391197\n",
      "Average loss at step  3308000 :  6.184131521940231\n",
      "Average loss at step  3310000 :  4.301324791431427\n",
      "Average loss at step  3312000 :  4.257715487480164\n",
      "Average loss at step  3314000 :  4.2980838075876235\n",
      "Average loss at step  3316000 :  4.2780614408254625\n",
      "Average loss at step  3318000 :  4.209757933020592\n",
      "Average loss at step  3320000 :  4.185253799319267\n",
      "Average loss at step  3322000 :  4.261738455176354\n",
      "Average loss at step  3324000 :  4.226805018186569\n",
      "Average loss at step  3326000 :  4.188318909764289\n",
      "Average loss at step  3328000 :  4.149495833516121\n",
      "Average loss at step  3330000 :  4.15300122988224\n",
      "Average loss at step  3332000 :  4.144390774846077\n",
      "Average loss at step  3334000 :  4.028588071584702\n",
      "Average loss at step  3336000 :  4.057021255493164\n",
      "Average loss at step  3338000 :  4.222104864358902\n",
      "Average loss at step  3340000 :  4.103942946910858\n",
      "Average loss at step  3342000 :  4.07567398238182\n",
      "Average loss at step  3344000 :  4.231583204269409\n",
      "Average loss at step  3346000 :  4.007851620316505\n",
      "Average loss at step  3348000 :  3.8682131521701812\n",
      "Average loss at step  3350000 :  4.126976022362709\n",
      "Average loss at step  3352000 :  4.232260905861855\n",
      "Average loss at step  3354000 :  4.150144571959973\n",
      "Average loss at step  3356000 :  4.036740766465664\n",
      "Average loss at step  3358000 :  4.265032622098922\n",
      "Average loss at step  3360000 :  4.2081700406074525\n",
      "Average loss at step  3362000 :  4.115070428252221\n",
      "Average loss at step  3364000 :  4.163466676712036\n",
      "Average loss at step  3366000 :  4.170073058128357\n",
      "Average loss at step  3368000 :  4.090682904958725\n",
      "Average loss at step  3370000 :  4.1562450128793715\n",
      "Average loss at step  3372000 :  4.288956211209297\n",
      "Average loss at step  3374000 :  4.193730337798596\n",
      "Average loss at step  3376000 :  4.114949045181274\n",
      "Average loss at step  3378000 :  4.215944210886955\n",
      "Average loss at step  3380000 :  4.218287520885467\n",
      "Average loss at step  3382000 :  3.971690453529358\n",
      "Average loss at step  3384000 :  3.8611881313323972\n",
      "Average loss at step  3386000 :  3.871299295514822\n",
      "Average loss at step  3388000 :  3.841913709282875\n",
      "Average loss at step  3390000 :  3.8526565753221513\n",
      "Average loss at step  3392000 :  6.582217209577561\n",
      "Average loss at step  3394000 :  4.238634726166725\n",
      "Average loss at step  3396000 :  4.191106496572495\n",
      "Average loss at step  3398000 :  4.2207773950099945\n",
      "Average loss at step  3400000 :  4.242916737079621\n",
      "Average loss at step  3402000 :  4.273023612141609\n",
      "Average loss at step  3404000 :  4.184775333881378\n",
      "Average loss at step  3406000 :  4.258305226564407\n",
      "Average loss at step  3408000 :  4.212826609075069\n",
      "Average loss at step  3410000 :  4.179594298362732\n",
      "Average loss at step  3412000 :  4.045183210134506\n",
      "Average loss at step  3414000 :  4.051167834043503\n",
      "Average loss at step  3416000 :  4.07749908387661\n",
      "Average loss at step  3418000 :  4.097150009036064\n",
      "Average loss at step  3420000 :  4.184056275486946\n",
      "Average loss at step  3422000 :  3.974365152478218\n",
      "Average loss at step  3424000 :  4.173874664783478\n",
      "Average loss at step  3426000 :  3.981801858305931\n",
      "Average loss at step  3428000 :  4.2651593343019485\n",
      "Average loss at step  3430000 :  4.213630584061145\n",
      "Average loss at step  3432000 :  3.863726140141487\n",
      "Average loss at step  3434000 :  4.171046970129013\n",
      "Average loss at step  3436000 :  4.2222324610948565\n",
      "Average loss at step  3438000 :  4.199720796108246\n",
      "Average loss at step  3440000 :  4.221764284729957\n",
      "Average loss at step  3442000 :  4.218494198083878\n",
      "Average loss at step  3444000 :  4.020856776475906\n",
      "Average loss at step  3446000 :  3.994647690653801\n",
      "Average loss at step  3448000 :  3.865129676580429\n",
      "Average loss at step  3450000 :  3.731894283413887\n",
      "Average loss at step  3452000 :  4.185287802457809\n",
      "Average loss at step  3454000 :  4.152408748626709\n",
      "Average loss at step  3456000 :  4.209210748791695\n",
      "Average loss at step  3458000 :  4.202649110555649\n",
      "Average loss at step  3460000 :  4.2070054696798325\n",
      "Average loss at step  3462000 :  4.217279461503029\n",
      "Average loss at step  3464000 :  4.140108359336853\n",
      "Average loss at step  3466000 :  4.277167080402374\n",
      "Average loss at step  3468000 :  4.1677658160924915\n",
      "Average loss at step  3470000 :  4.126471786528826\n",
      "Average loss at step  3472000 :  3.709360061883926\n",
      "Average loss at step  3474000 :  4.177710222005844\n",
      "Average loss at step  3476000 :  4.230217252373695\n",
      "Average loss at step  3478000 :  4.186038895905018\n",
      "Average loss at step  3480000 :  4.235042425274849\n",
      "Average loss at step  3482000 :  4.0122421292066575\n",
      "Average loss at step  3484000 :  3.8202806191444396\n",
      "Average loss at step  3486000 :  3.6228664292097092\n",
      "Average loss at step  3488000 :  3.7803578835725786\n",
      "Average loss at step  3490000 :  3.802963869571686\n",
      "Average loss at step  3492000 :  4.315372075796128\n",
      "Average loss at step  3494000 :  4.155868555545807\n",
      "Average loss at step  3496000 :  4.239533702850342\n",
      "Average loss at step  3498000 :  4.223867606043815\n",
      "Average loss at step  3500000 :  4.2378089159727095\n",
      "Average loss at step  3502000 :  4.219829594612121\n",
      "Average loss at step  3504000 :  4.22535447883606\n",
      "Average loss at step  3506000 :  4.214189233183861\n",
      "Average loss at step  3508000 :  4.200078393936157\n",
      "Average loss at step  3510000 :  4.265410857081413\n",
      "Average loss at step  3512000 :  4.179753135085106\n",
      "Average loss at step  3514000 :  4.2121195667982105\n",
      "Average loss at step  3516000 :  4.110872175335884\n",
      "Average loss at step  3518000 :  4.135609905004501\n",
      "Average loss at step  3520000 :  4.171430785417557\n",
      "Average loss at step  3522000 :  4.163611436009407\n",
      "Average loss at step  3524000 :  4.116780079126358\n",
      "Average loss at step  3526000 :  5.07238287627697\n",
      "Average loss at step  3528000 :  4.177277361869812\n",
      "Average loss at step  3530000 :  4.173160148024559\n",
      "Average loss at step  3532000 :  4.071945364117623\n",
      "Average loss at step  3534000 :  3.847249962925911\n",
      "Average loss at step  3536000 :  3.8283918282985687\n",
      "Average loss at step  3538000 :  3.9566961290836336\n",
      "Average loss at step  3540000 :  3.9912461191415787\n",
      "Average loss at step  3542000 :  3.831905724167824\n",
      "Average loss at step  3544000 :  3.882674755990505\n",
      "Average loss at step  3546000 :  4.33382795804739\n",
      "Average loss at step  3548000 :  4.2433984438180925\n",
      "Average loss at step  3550000 :  4.143917454808951\n",
      "Average loss at step  3552000 :  4.306766363739968\n",
      "Average loss at step  3554000 :  4.2082082161903385\n",
      "Average loss at step  3556000 :  4.122085989236831\n",
      "Average loss at step  3558000 :  4.165790180206299\n",
      "Average loss at step  3560000 :  4.028332425177097\n",
      "Average loss at step  3562000 :  4.925714789748191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  3564000 :  4.0770897070169445\n",
      "Average loss at step  3566000 :  4.152059184253216\n",
      "Average loss at step  3568000 :  4.214602373361587\n",
      "Average loss at step  3570000 :  4.327242448925972\n",
      "Average loss at step  3572000 :  4.2528468831777575\n",
      "Average loss at step  3574000 :  4.32986822283268\n",
      "Average loss at step  3576000 :  4.2931571506261825\n",
      "Average loss at step  3578000 :  4.2346339802742\n",
      "Average loss at step  3580000 :  4.22492806661129\n",
      "Average loss at step  3582000 :  4.177823325157165\n",
      "Average loss at step  3584000 :  4.2161992925405505\n",
      "Average loss at step  3586000 :  4.482038970708847\n",
      "Average loss at step  3588000 :  4.294186702489853\n",
      "Average loss at step  3590000 :  4.180629605889321\n",
      "Average loss at step  3592000 :  4.11440002477169\n",
      "Average loss at step  3594000 :  4.103549919009208\n",
      "Average loss at step  3596000 :  4.115594978451729\n",
      "Average loss at step  3598000 :  4.216429683923721\n",
      "Average loss at step  3600000 :  4.275433346390725\n",
      "Average loss at step  3602000 :  4.149577519416809\n",
      "Average loss at step  3604000 :  4.133447773933411\n",
      "Average loss at step  3606000 :  4.088741576313972\n",
      "Average loss at step  3608000 :  4.239126192450524\n",
      "Average loss at step  3610000 :  3.9854557613283395\n",
      "Average loss at step  3612000 :  3.820286252140999\n",
      "Average loss at step  3614000 :  4.102567333698273\n",
      "Average loss at step  3616000 :  4.20665255856514\n",
      "Average loss at step  3618000 :  4.156141013145446\n",
      "Average loss at step  3620000 :  3.6732070553302765\n",
      "Average loss at step  3622000 :  4.353442125082016\n",
      "Average loss at step  3624000 :  4.149624638676643\n",
      "Average loss at step  3626000 :  4.231352477788925\n",
      "Average loss at step  3628000 :  4.17956250500679\n",
      "Average loss at step  3630000 :  4.218837259888649\n",
      "Average loss at step  3632000 :  4.227949494123459\n",
      "Average loss at step  3634000 :  4.180986719250679\n",
      "Average loss at step  3636000 :  4.095828777432442\n",
      "Average loss at step  3638000 :  3.9969525274038316\n",
      "Average loss at step  3640000 :  4.23603399014473\n",
      "Average loss at step  3642000 :  4.153546925663948\n",
      "Average loss at step  3644000 :  4.348958222508431\n",
      "Average loss at step  3646000 :  4.249591330647468\n",
      "Average loss at step  3648000 :  4.211759130954743\n",
      "Average loss at step  3650000 :  4.192739612936974\n",
      "Average loss at step  3652000 :  4.150738647699356\n",
      "Average loss at step  3654000 :  4.10544367814064\n",
      "Average loss at step  3656000 :  4.044905759453774\n",
      "Average loss at step  3658000 :  4.092989506363868\n",
      "Average loss at step  3660000 :  4.122572349369526\n",
      "Average loss at step  3662000 :  4.267873309373855\n",
      "Average loss at step  3664000 :  4.13779400998354\n",
      "Average loss at step  3666000 :  4.151558290600777\n",
      "Average loss at step  3668000 :  4.202427136898041\n",
      "Average loss at step  3670000 :  4.216026928424835\n",
      "Average loss at step  3672000 :  4.2451441782712935\n",
      "Average loss at step  3674000 :  4.156735509634018\n",
      "Average loss at step  3676000 :  4.142752398490906\n",
      "Average loss at step  3678000 :  4.153612752318383\n",
      "Average loss at step  3680000 :  4.278401515483856\n",
      "Average loss at step  3682000 :  4.2688597160577775\n",
      "Average loss at step  3684000 :  4.241512173414231\n",
      "Average loss at step  3686000 :  4.155025640487671\n",
      "Average loss at step  3688000 :  4.203854812026024\n",
      "Average loss at step  3690000 :  4.272562776684761\n",
      "Average loss at step  3692000 :  4.185606191635132\n",
      "Average loss at step  3694000 :  4.216439091563225\n",
      "Average loss at step  3696000 :  4.208018691658974\n",
      "Average loss at step  3698000 :  4.107855884194374\n",
      "Average loss at step  3700000 :  4.168306455731392\n",
      "Average loss at step  3702000 :  4.152546735882759\n",
      "Average loss at step  3704000 :  4.149897114515305\n",
      "Average loss at step  3706000 :  4.192969975173473\n",
      "Average loss at step  3708000 :  4.300626600623131\n",
      "Average loss at step  3710000 :  4.232437736272812\n",
      "Average loss at step  3712000 :  4.204153217673301\n",
      "Average loss at step  3714000 :  4.198625792264938\n",
      "Average loss at step  3716000 :  4.208433878302574\n",
      "Average loss at step  3718000 :  4.1824404078722\n",
      "Average loss at step  3720000 :  4.126215286493301\n",
      "Average loss at step  3722000 :  4.142204295158386\n",
      "Average loss at step  3724000 :  4.161384542346001\n",
      "Average loss at step  3726000 :  4.120833258390427\n",
      "Average loss at step  3728000 :  4.13757514166832\n",
      "Average loss at step  3730000 :  4.132708936214447\n",
      "Average loss at step  3732000 :  4.185336724042893\n",
      "Average loss at step  3734000 :  3.9883602896034716\n",
      "Average loss at step  3736000 :  4.26150810354948\n",
      "Average loss at step  3738000 :  4.136107677578926\n",
      "Average loss at step  3740000 :  4.30524528503418\n",
      "Average loss at step  3742000 :  4.193557836771012\n",
      "Average loss at step  3744000 :  4.2243393253088\n",
      "Average loss at step  3746000 :  4.1665162025690075\n",
      "Average loss at step  3748000 :  4.112775901556015\n",
      "Average loss at step  3750000 :  4.0944943242073055\n",
      "Average loss at step  3752000 :  4.249568683505058\n",
      "Average loss at step  3754000 :  4.3063661150932315\n",
      "Average loss at step  3756000 :  4.123060441970825\n",
      "Average loss at step  3758000 :  4.138112129688263\n",
      "Average loss at step  3760000 :  4.208101487755775\n",
      "Average loss at step  3762000 :  4.185056493699551\n",
      "Average loss at step  3764000 :  4.215464870333672\n",
      "Average loss at step  3766000 :  4.192066262722015\n",
      "Average loss at step  3768000 :  4.227452212095261\n",
      "Average loss at step  3770000 :  4.175464283466339\n",
      "Average loss at step  3772000 :  4.197513023972511\n",
      "Average loss at step  3774000 :  4.171800206661224\n",
      "Average loss at step  3776000 :  4.1583171820640565\n",
      "Average loss at step  3778000 :  3.791035192787647\n",
      "Average loss at step  3780000 :  3.9492722492814063\n",
      "Average loss at step  3782000 :  4.239598187685012\n",
      "Average loss at step  3784000 :  4.200963158369064\n",
      "Average loss at step  3786000 :  4.082023727059364\n",
      "Average loss at step  3788000 :  4.165686719298363\n",
      "Average loss at step  3790000 :  4.245241846799851\n",
      "Average loss at step  3792000 :  4.075020703077317\n",
      "Average loss at step  3794000 :  4.182647200226784\n",
      "Average loss at step  3796000 :  4.194740807890892\n",
      "Average loss at step  3798000 :  4.154475911259651\n",
      "Average loss at step  3800000 :  4.1447050192356105\n",
      "Average loss at step  3802000 :  4.107507128834724\n",
      "Average loss at step  3804000 :  4.104504588365555\n",
      "Average loss at step  3806000 :  4.141290336847305\n",
      "Average loss at step  3808000 :  4.099269788861275\n",
      "Average loss at step  3810000 :  4.13832110953331\n",
      "Average loss at step  3812000 :  4.193791010975838\n",
      "Average loss at step  3814000 :  4.273154623746872\n",
      "Average loss at step  3816000 :  4.202361536860466\n",
      "Average loss at step  3818000 :  4.157084587454796\n",
      "Average loss at step  3820000 :  4.087386882066727\n",
      "Average loss at step  3822000 :  4.044961997389794\n",
      "Average loss at step  3824000 :  4.087927735924721\n",
      "Average loss at step  3826000 :  4.054113995194435\n",
      "Average loss at step  3828000 :  3.999554393053055\n",
      "Average loss at step  3830000 :  4.148970546603203\n",
      "Average loss at step  3832000 :  4.1922092958688735\n",
      "Average loss at step  3834000 :  4.200208195090294\n",
      "Average loss at step  3836000 :  4.201383264541626\n",
      "Average loss at step  3838000 :  4.191374695181847\n",
      "Average loss at step  3840000 :  4.152715491652489\n",
      "Average loss at step  3842000 :  4.182033527135849\n",
      "Average loss at step  3844000 :  4.1707316443920135\n",
      "Average loss at step  3846000 :  4.206660799026489\n",
      "Average loss at step  3848000 :  4.193445322155952\n",
      "Average loss at step  3850000 :  4.217234247803688\n",
      "Average loss at step  3852000 :  4.187242886424064\n",
      "Average loss at step  3854000 :  4.186630257248878\n",
      "Average loss at step  3856000 :  4.177693969845771\n",
      "Average loss at step  3858000 :  4.262518052816391\n",
      "Average loss at step  3860000 :  4.22249241411686\n",
      "Average loss at step  3862000 :  4.189013761997223\n",
      "Average loss at step  3864000 :  4.185126790285111\n",
      "Average loss at step  3866000 :  4.259120377898216\n",
      "Average loss at step  3868000 :  4.253946490287781\n",
      "Average loss at step  3870000 :  4.203843795657158\n",
      "Average loss at step  3872000 :  4.180474812507629\n",
      "Average loss at step  3874000 :  3.939067204117775\n",
      "Average loss at step  3876000 :  3.8571003262996673\n",
      "Average loss at step  3878000 :  4.282149348974228\n",
      "Average loss at step  3880000 :  4.210324085235595\n",
      "Average loss at step  3882000 :  4.211038458228111\n",
      "Average loss at step  3884000 :  4.185081494212151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  3886000 :  3.959756120085716\n",
      "Average loss at step  3888000 :  4.220717472195625\n",
      "Average loss at step  3890000 :  4.435142289400101\n",
      "Average loss at step  3892000 :  4.258772900342941\n",
      "Average loss at step  3894000 :  4.220959267556667\n",
      "Average loss at step  3896000 :  4.103786422967911\n",
      "Average loss at step  3898000 :  4.02489415872097\n",
      "Average loss at step  3900000 :  4.216237216591835\n",
      "Average loss at step  3902000 :  4.158270512580872\n",
      "Average loss at step  3904000 :  4.185557514190674\n",
      "Average loss at step  3906000 :  4.117013378858566\n",
      "Average loss at step  3908000 :  4.11818995654583\n",
      "Average loss at step  3910000 :  4.264666528344154\n",
      "Average loss at step  3912000 :  4.17735260272026\n",
      "Average loss at step  3914000 :  4.172715906262398\n",
      "Average loss at step  3916000 :  4.175023621082306\n",
      "Average loss at step  3918000 :  4.17156510579586\n",
      "Average loss at step  3920000 :  4.104652265071869\n",
      "Average loss at step  3922000 :  4.181418460488319\n",
      "Average loss at step  3924000 :  4.290337265253067\n",
      "Average loss at step  3926000 :  4.252861122012138\n",
      "Average loss at step  3928000 :  4.242446013808251\n",
      "Average loss at step  3930000 :  4.173941988945007\n",
      "Average loss at step  3932000 :  4.219884052753448\n",
      "Average loss at step  3934000 :  4.2028902541399\n",
      "Average loss at step  3936000 :  4.207152617812157\n",
      "Average loss at step  3938000 :  4.24888331925869\n",
      "Average loss at step  3940000 :  4.258708262681961\n",
      "Average loss at step  3942000 :  4.200309076905251\n",
      "Average loss at step  3944000 :  4.191172778129578\n",
      "Average loss at step  3946000 :  4.12742778301239\n",
      "Average loss at step  3948000 :  4.095806774020195\n",
      "Average loss at step  3950000 :  4.092951966524124\n",
      "Average loss at step  3952000 :  4.084185032486916\n",
      "Average loss at step  3954000 :  4.04483733868599\n",
      "Average loss at step  3956000 :  4.064787200450898\n",
      "Average loss at step  3958000 :  4.060607463121414\n",
      "Average loss at step  3960000 :  4.0394546579122546\n",
      "Average loss at step  3962000 :  4.049370361685753\n",
      "Average loss at step  3964000 :  4.079770349740982\n",
      "Average loss at step  3966000 :  4.038605590701104\n",
      "Average loss at step  3968000 :  4.058060800671577\n",
      "Average loss at step  3970000 :  4.065040317058563\n",
      "Average loss at step  3972000 :  4.2809110354185105\n",
      "Average loss at step  3974000 :  4.0077203555107115\n",
      "Average loss at step  3976000 :  3.9231920541524885\n",
      "Average loss at step  3978000 :  3.9106103405952455\n",
      "Average loss at step  3980000 :  4.0537941390872\n",
      "Average loss at step  3982000 :  4.2859871184825895\n",
      "Average loss at step  3984000 :  4.162735064983368\n",
      "Average loss at step  3986000 :  4.153903095364571\n",
      "Average loss at step  3988000 :  4.091785836696625\n",
      "Average loss at step  3990000 :  4.033431241989136\n",
      "Average loss at step  3992000 :  4.137642609059811\n",
      "Average loss at step  3994000 :  4.057865930914879\n",
      "Average loss at step  3996000 :  4.2126051498651504\n",
      "Average loss at step  3998000 :  4.141261387050152\n",
      "Average loss at step  4000000 :  4.272943365573883\n",
      "Average loss at step  4002000 :  4.1898700160980225\n",
      "Average loss at step  4004000 :  4.182476890206337\n",
      "Average loss at step  4006000 :  4.132399782896042\n",
      "Average loss at step  4008000 :  3.845476448774338\n",
      "Average loss at step  4010000 :  4.100869482278823\n",
      "Average loss at step  4012000 :  3.9308241386413574\n",
      "Average loss at step  4014000 :  4.0960865789651875\n",
      "Average loss at step  4016000 :  4.007548435688019\n",
      "Average loss at step  4018000 :  3.9229548910856247\n",
      "Average loss at step  4020000 :  3.963465995788574\n",
      "Average loss at step  4022000 :  4.13278230547905\n",
      "Average loss at step  4024000 :  4.237964168190956\n",
      "Average loss at step  4026000 :  4.0195339730978015\n",
      "Average loss at step  4028000 :  3.9772537362575533\n",
      "Average loss at step  4030000 :  4.05517911195755\n",
      "Average loss at step  4032000 :  4.054121197700501\n",
      "Average loss at step  4034000 :  4.151982319235802\n",
      "Average loss at step  4036000 :  4.097617628335953\n",
      "Average loss at step  4038000 :  4.2194542282819745\n",
      "Average loss at step  4040000 :  4.285480905056\n",
      "Average loss at step  4042000 :  4.144972797393799\n",
      "Average loss at step  4044000 :  4.227373873144388\n",
      "Average loss at step  4046000 :  4.165823172092438\n",
      "Average loss at step  4048000 :  4.166446266531945\n",
      "Average loss at step  4050000 :  4.071553066134453\n",
      "Average loss at step  4052000 :  4.09922862136364\n",
      "Average loss at step  4054000 :  3.987619666337967\n",
      "Average loss at step  4056000 :  4.124542021393776\n",
      "Average loss at step  4058000 :  4.12136102938652\n",
      "Average loss at step  4060000 :  3.9506896184682847\n",
      "Average loss at step  4062000 :  4.291863277196884\n",
      "Average loss at step  4064000 :  4.515575767040253\n",
      "Average loss at step  4066000 :  4.200910571098327\n",
      "Average loss at step  4068000 :  4.203286252975464\n",
      "Average loss at step  4070000 :  3.9386367959976196\n",
      "Average loss at step  4072000 :  3.9542055374383924\n",
      "Average loss at step  4074000 :  3.8984641669988633\n",
      "Average loss at step  4076000 :  3.869905239880085\n",
      "Average loss at step  4078000 :  3.9109919483661653\n",
      "Average loss at step  4080000 :  3.953674148440361\n",
      "Average loss at step  4082000 :  4.0044738020896915\n",
      "Average loss at step  4084000 :  4.206432233214378\n",
      "Average loss at step  4086000 :  4.200227770805359\n",
      "Average loss at step  4088000 :  4.257225731134414\n",
      "Average loss at step  4090000 :  4.243572308897972\n",
      "Average loss at step  4092000 :  4.226712932348251\n",
      "Average loss at step  4094000 :  4.0566117603778835\n",
      "Average loss at step  4096000 :  4.157467842817306\n",
      "Average loss at step  4098000 :  4.143664270281792\n",
      "Average loss at step  4100000 :  4.04044417977333\n",
      "Average loss at step  4102000 :  4.390775866538286\n",
      "Average loss at step  4104000 :  3.9479009630680086\n",
      "Average loss at step  4106000 :  3.8106751450300216\n",
      "Average loss at step  4108000 :  3.7630007705688477\n",
      "Average loss at step  4110000 :  4.29995552611351\n",
      "Average loss at step  4112000 :  4.214617772579193\n",
      "Average loss at step  4114000 :  4.226189399123192\n",
      "Average loss at step  4116000 :  4.181467559933663\n",
      "Average loss at step  4118000 :  4.211935299754143\n",
      "Average loss at step  4120000 :  4.205085269331932\n",
      "Average loss at step  4122000 :  4.146591173171997\n",
      "Average loss at step  4124000 :  4.205237092494965\n",
      "Average loss at step  4126000 :  4.033274859428405\n",
      "Average loss at step  4128000 :  4.111019976615906\n",
      "Average loss at step  4130000 :  4.189773389935493\n",
      "Average loss at step  4132000 :  3.9300205536186694\n",
      "Average loss at step  4134000 :  4.226871422886848\n",
      "Average loss at step  4136000 :  4.2869274888038635\n",
      "Average loss at step  4138000 :  4.2773903203010555\n",
      "Average loss at step  4140000 :  3.6982981644272805\n",
      "Average loss at step  4142000 :  3.823521225869656\n",
      "Average loss at step  4144000 :  4.183079451680183\n",
      "Average loss at step  4146000 :  4.185482981801033\n",
      "Average loss at step  4148000 :  4.113695110440254\n",
      "Average loss at step  4150000 :  4.1259934700131415\n",
      "Average loss at step  4152000 :  4.065874821424484\n",
      "Average loss at step  4154000 :  4.261680148363113\n",
      "Average loss at step  4156000 :  4.202968313217163\n",
      "Average loss at step  4158000 :  4.111506407141685\n",
      "Average loss at step  4160000 :  4.174053936839104\n",
      "Average loss at step  4162000 :  4.119271915793419\n",
      "Average loss at step  4164000 :  4.0704442610740665\n",
      "Average loss at step  4166000 :  4.194694611698389\n",
      "Average loss at step  4168000 :  4.286839473247528\n",
      "Average loss at step  4170000 :  4.148448015332222\n",
      "Average loss at step  4172000 :  4.16079462993145\n",
      "Average loss at step  4174000 :  4.229728929162025\n",
      "Average loss at step  4176000 :  4.212490083456039\n",
      "Average loss at step  4178000 :  4.262913327932358\n",
      "Average loss at step  4180000 :  4.2001431730985646\n",
      "Average loss at step  4182000 :  4.168229507923126\n",
      "Average loss at step  4184000 :  4.2768593693971635\n",
      "Average loss at step  4186000 :  4.2544221552610395\n",
      "Average loss at step  4188000 :  4.177647365450859\n",
      "Average loss at step  4190000 :  4.1765509090423585\n",
      "Average loss at step  4192000 :  4.182848560214043\n",
      "Average loss at step  4194000 :  4.255456571698189\n",
      "Average loss at step  4196000 :  4.15864793920517\n",
      "Average loss at step  4198000 :  4.088380089998245\n",
      "Average loss at step  4200000 :  4.2520553590059285\n",
      "Average loss at step  4202000 :  4.128512553095818\n",
      "Average loss at step  4204000 :  4.259213805198669\n",
      "Average loss at step  4206000 :  4.212350514769554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  4208000 :  4.18959337925911\n",
      "Average loss at step  4210000 :  4.375190306723118\n",
      "Average loss at step  4212000 :  4.230963682651519\n",
      "Average loss at step  4214000 :  4.216729292154312\n",
      "Average loss at step  4216000 :  4.280466813325882\n",
      "Average loss at step  4218000 :  4.261986058831215\n",
      "Average loss at step  4220000 :  4.190560103058815\n",
      "Average loss at step  4222000 :  4.155306111931801\n",
      "Average loss at step  4224000 :  4.226473331809044\n",
      "Average loss at step  4226000 :  4.215585485816002\n",
      "Average loss at step  4228000 :  4.1666925798654555\n",
      "Average loss at step  4230000 :  4.119848558306694\n",
      "Average loss at step  4232000 :  4.148442395567894\n",
      "Average loss at step  4234000 :  4.141288810610771\n",
      "Average loss at step  4236000 :  4.030060844838619\n",
      "Average loss at step  4238000 :  4.035449810624122\n",
      "Average loss at step  4240000 :  4.200327381014824\n",
      "Average loss at step  4242000 :  4.137929275155067\n",
      "Average loss at step  4244000 :  4.044119675159454\n",
      "Average loss at step  4246000 :  4.1183430734872815\n",
      "Average loss at step  4248000 :  4.014336015045643\n",
      "Average loss at step  4250000 :  3.848287817239761\n",
      "Average loss at step  4252000 :  4.062795657575131\n",
      "Average loss at step  4254000 :  4.2359258954525\n",
      "Average loss at step  4256000 :  4.139122321724892\n",
      "Average loss at step  4258000 :  4.160256576538086\n",
      "Average loss at step  4260000 :  4.108343552112579\n",
      "Average loss at step  4262000 :  4.197653460741043\n",
      "Average loss at step  4264000 :  4.121347739577294\n",
      "Average loss at step  4266000 :  4.135304266214371\n",
      "Average loss at step  4268000 :  4.158645589470863\n",
      "Average loss at step  4270000 :  4.09114100253582\n",
      "Average loss at step  4272000 :  4.123492967963219\n",
      "Average loss at step  4274000 :  4.289853019237518\n",
      "Average loss at step  4276000 :  4.163081518411636\n",
      "Average loss at step  4278000 :  4.129781339168549\n",
      "Average loss at step  4280000 :  4.20843898165226\n",
      "Average loss at step  4282000 :  4.270377173066139\n",
      "Average loss at step  4284000 :  3.9652588827610016\n",
      "Average loss at step  4286000 :  3.865750818103552\n",
      "Average loss at step  4288000 :  3.830253013879061\n",
      "Average loss at step  4290000 :  3.859079600274563\n",
      "Average loss at step  4292000 :  3.840076991558075\n",
      "Average loss at step  4294000 :  4.1747015554308895\n",
      "Average loss at step  4296000 :  5.589040775179863\n",
      "Average loss at step  4298000 :  4.175563413143158\n",
      "Average loss at step  4300000 :  4.203401805818081\n",
      "Average loss at step  4302000 :  4.338896445780993\n",
      "Average loss at step  4304000 :  4.570599149107933\n",
      "Average loss at step  4306000 :  4.2081513934135435\n",
      "Average loss at step  4308000 :  4.242146213054657\n",
      "Average loss at step  4310000 :  4.210438915252686\n",
      "Average loss at step  4312000 :  4.247860961794853\n",
      "Average loss at step  4314000 :  4.023038484454155\n",
      "Average loss at step  4316000 :  4.036823023438454\n",
      "Average loss at step  4318000 :  4.052686739206314\n",
      "Average loss at step  4320000 :  4.102140543937683\n",
      "Average loss at step  4322000 :  4.157620272994041\n",
      "Average loss at step  4324000 :  4.016652813315392\n",
      "Average loss at step  4326000 :  4.094535555958748\n",
      "Average loss at step  4328000 :  3.972046681791544\n",
      "Average loss at step  4330000 :  4.2611856508255\n",
      "Average loss at step  4332000 :  4.177356050312519\n",
      "Average loss at step  4334000 :  3.814627067953348\n",
      "Average loss at step  4336000 :  4.165581753373146\n",
      "Average loss at step  4338000 :  4.1718925731182095\n",
      "Average loss at step  4340000 :  4.204455657482147\n",
      "Average loss at step  4342000 :  4.1838387371301655\n",
      "Average loss at step  4344000 :  4.2086266452074055\n",
      "Average loss at step  4346000 :  4.043685953617096\n",
      "Average loss at step  4348000 :  3.9758990573883057\n",
      "Average loss at step  4350000 :  3.8820962499380114\n",
      "Average loss at step  4352000 :  3.7453437410593033\n",
      "Average loss at step  4354000 :  4.082534433126449\n",
      "Average loss at step  4356000 :  4.1663158552646635\n",
      "Average loss at step  4358000 :  4.200590445280075\n",
      "Average loss at step  4360000 :  4.194697037577629\n",
      "Average loss at step  4362000 :  4.207966623485088\n",
      "Average loss at step  4364000 :  4.213468347668647\n",
      "Average loss at step  4366000 :  4.120655045509339\n",
      "Average loss at step  4368000 :  4.09601483976841\n",
      "Average loss at step  4370000 :  4.120358600735664\n",
      "Average loss at step  4372000 :  4.1741869417726996\n",
      "Average loss at step  4374000 :  3.6227600326538085\n",
      "Average loss at step  4376000 :  4.168575156092643\n",
      "Average loss at step  4378000 :  4.247061757087708\n",
      "Average loss at step  4380000 :  4.195109761118889\n",
      "Average loss at step  4382000 :  4.239745612502098\n",
      "Average loss at step  4384000 :  3.983992739021778\n",
      "Average loss at step  4386000 :  3.8480111317634584\n",
      "Average loss at step  4388000 :  3.68467040348053\n",
      "Average loss at step  4390000 :  3.724651510596275\n",
      "Average loss at step  4392000 :  3.794464106798172\n",
      "Average loss at step  4394000 :  4.228561676502228\n",
      "Average loss at step  4396000 :  4.159465035915375\n",
      "Average loss at step  4398000 :  4.224099647164345\n",
      "Average loss at step  4400000 :  4.20309801363945\n",
      "Average loss at step  4402000 :  4.234095519661904\n",
      "Average loss at step  4404000 :  4.205250571370125\n",
      "Average loss at step  4406000 :  4.221819942593575\n",
      "Average loss at step  4408000 :  4.219459772467613\n",
      "Average loss at step  4410000 :  4.188717906594277\n",
      "Average loss at step  4412000 :  4.260811293005943\n",
      "Average loss at step  4414000 :  4.177416767954826\n",
      "Average loss at step  4416000 :  4.21128894329071\n",
      "Average loss at step  4418000 :  4.112855588912963\n",
      "Average loss at step  4420000 :  4.079122620701789\n",
      "Average loss at step  4422000 :  4.20851875948906\n",
      "Average loss at step  4424000 :  4.196178339004517\n",
      "Average loss at step  4426000 :  4.086878829360009\n",
      "Average loss at step  4428000 :  4.152618668913841\n",
      "Average loss at step  4430000 :  4.305553072452545\n",
      "Average loss at step  4432000 :  4.113348856955767\n",
      "Average loss at step  4434000 :  4.02404675757885\n",
      "Average loss at step  4436000 :  3.840325150728226\n",
      "Average loss at step  4438000 :  3.7778602638244627\n",
      "Average loss at step  4440000 :  3.9167122733592987\n",
      "Average loss at step  4442000 :  3.9334159483909605\n",
      "Average loss at step  4444000 :  3.825208591341972\n",
      "Average loss at step  4446000 :  3.9026695528030397\n",
      "Average loss at step  4448000 :  4.235145609378815\n",
      "Average loss at step  4450000 :  4.232803377270699\n",
      "Average loss at step  4452000 :  4.1210980470478535\n",
      "Average loss at step  4454000 :  4.290809237360954\n",
      "Average loss at step  4456000 :  4.226190590023995\n",
      "Average loss at step  4458000 :  4.103069730401039\n",
      "Average loss at step  4460000 :  4.135655307531357\n",
      "Average loss at step  4462000 :  4.055457982987165\n",
      "Average loss at step  4464000 :  4.670923736810685\n",
      "Average loss at step  4466000 :  4.074486429333687\n",
      "Average loss at step  4468000 :  4.143380655288697\n",
      "Average loss at step  4470000 :  4.134971055150032\n",
      "Average loss at step  4472000 :  4.306402823448181\n",
      "Average loss at step  4474000 :  4.242199401974678\n",
      "Average loss at step  4476000 :  4.270020652294159\n",
      "Average loss at step  4478000 :  4.260360074937344\n",
      "Average loss at step  4480000 :  4.216417543292046\n",
      "Average loss at step  4482000 :  4.205303195357323\n",
      "Average loss at step  4484000 :  4.15982526564598\n",
      "Average loss at step  4486000 :  4.2029476293325425\n",
      "Average loss at step  4488000 :  4.265551873922348\n",
      "Average loss at step  4490000 :  4.28599813246727\n",
      "Average loss at step  4492000 :  4.156330735445023\n",
      "Average loss at step  4494000 :  4.090988608002663\n",
      "Average loss at step  4496000 :  4.096594159841538\n",
      "Average loss at step  4498000 :  4.105422173142433\n",
      "Average loss at step  4500000 :  4.187149691939354\n",
      "Average loss at step  4502000 :  4.226226090550423\n",
      "Average loss at step  4504000 :  4.1322480311393734\n",
      "Average loss at step  4506000 :  4.118514039039612\n",
      "Average loss at step  4508000 :  4.077282511234284\n",
      "Average loss at step  4510000 :  4.195967168211937\n",
      "Average loss at step  4512000 :  4.034226320117712\n",
      "Average loss at step  4514000 :  3.7989291998147965\n",
      "Average loss at step  4516000 :  4.024314405202865\n",
      "Average loss at step  4518000 :  4.207740723490715\n",
      "Average loss at step  4520000 :  4.144069926738739\n",
      "Average loss at step  4522000 :  3.607293146312237\n",
      "Average loss at step  4524000 :  4.376386913180351\n",
      "Average loss at step  4526000 :  4.153105731487274\n",
      "Average loss at step  4528000 :  4.212549161791801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  4530000 :  4.149752880573272\n",
      "Average loss at step  4532000 :  4.1846891183853145\n",
      "Average loss at step  4534000 :  4.210635807871818\n",
      "Average loss at step  4536000 :  4.168481040239334\n",
      "Average loss at step  4538000 :  4.087898511469364\n",
      "Average loss at step  4540000 :  3.9526787291765215\n",
      "Average loss at step  4542000 :  4.253226108193397\n",
      "Average loss at step  4544000 :  4.150192819237709\n",
      "Average loss at step  4546000 :  4.24141846370697\n",
      "Average loss at step  4548000 :  4.281536524534226\n",
      "Average loss at step  4550000 :  4.183938649535179\n",
      "Average loss at step  4552000 :  4.194679790616036\n",
      "Average loss at step  4554000 :  4.132886512160301\n",
      "Average loss at step  4556000 :  4.102761484861374\n",
      "Average loss at step  4558000 :  4.043122527241707\n",
      "Average loss at step  4560000 :  4.090080385327339\n",
      "Average loss at step  4562000 :  4.052859813511372\n",
      "Average loss at step  4564000 :  4.242203338384629\n",
      "Average loss at step  4566000 :  4.193543463706971\n",
      "Average loss at step  4568000 :  4.115847520947456\n",
      "Average loss at step  4570000 :  4.193356137096882\n",
      "Average loss at step  4572000 :  4.150139955878258\n",
      "Average loss at step  4574000 :  4.262524613142014\n",
      "Average loss at step  4576000 :  4.107232465147972\n",
      "Average loss at step  4578000 :  4.163907895326615\n",
      "Average loss at step  4580000 :  4.110780861616135\n",
      "Average loss at step  4582000 :  4.275246589183808\n",
      "Average loss at step  4584000 :  4.263834252476692\n",
      "Average loss at step  4586000 :  4.228268699049949\n",
      "Average loss at step  4588000 :  4.113771531641484\n",
      "Average loss at step  4590000 :  4.1975023614168165\n",
      "Average loss at step  4592000 :  4.27898507142067\n",
      "Average loss at step  4594000 :  4.187143523335457\n",
      "Average loss at step  4596000 :  4.214315096497535\n",
      "Average loss at step  4598000 :  4.165629113793373\n",
      "Average loss at step  4600000 :  4.115813590109348\n",
      "Average loss at step  4602000 :  4.175616080403328\n",
      "Average loss at step  4604000 :  4.152833451509475\n",
      "Average loss at step  4606000 :  4.110979317128658\n",
      "Average loss at step  4608000 :  4.171212138593197\n",
      "Average loss at step  4610000 :  4.29861800968647\n",
      "Average loss at step  4612000 :  4.226418726444244\n",
      "Average loss at step  4614000 :  4.196693412780761\n",
      "Average loss at step  4616000 :  4.196123433828354\n",
      "Average loss at step  4618000 :  4.188140453100204\n",
      "Average loss at step  4620000 :  4.18403773522377\n",
      "Average loss at step  4622000 :  4.127689279079437\n",
      "Average loss at step  4624000 :  4.110866922020912\n",
      "Average loss at step  4626000 :  4.172920532941818\n",
      "Average loss at step  4628000 :  4.134008439302445\n",
      "Average loss at step  4630000 :  4.1179424431324\n",
      "Average loss at step  4632000 :  4.103955040693283\n",
      "Average loss at step  4634000 :  4.171608687400818\n",
      "Average loss at step  4636000 :  3.973319745570421\n",
      "Average loss at step  4638000 :  4.219380363404751\n",
      "Average loss at step  4640000 :  4.1288332138061525\n",
      "Average loss at step  4642000 :  4.297244509100914\n",
      "Average loss at step  4644000 :  4.213452514529228\n",
      "Average loss at step  4646000 :  4.200796384215355\n",
      "Average loss at step  4648000 :  4.178384867787361\n",
      "Average loss at step  4650000 :  4.111977411866188\n",
      "Average loss at step  4652000 :  4.086008188486099\n",
      "Average loss at step  4654000 :  4.221412325739861\n",
      "Average loss at step  4656000 :  4.3907963398695\n",
      "Average loss at step  4658000 :  4.140436323285103\n",
      "Average loss at step  4660000 :  4.140440168619156\n",
      "Average loss at step  4662000 :  4.1543442541360855\n",
      "Average loss at step  4664000 :  4.184307433396578\n",
      "Average loss at step  4666000 :  4.209530799627304\n",
      "Average loss at step  4668000 :  4.170775329232216\n",
      "Average loss at step  4670000 :  4.243850372672081\n",
      "Average loss at step  4672000 :  4.180120416402817\n",
      "Average loss at step  4674000 :  4.163443748354911\n",
      "Average loss at step  4676000 :  4.191470828413963\n",
      "Average loss at step  4678000 :  4.145499224424362\n",
      "Average loss at step  4680000 :  3.8880789217948912\n",
      "Average loss at step  4682000 :  3.712355822145939\n",
      "Average loss at step  4684000 :  4.248989538788796\n",
      "Average loss at step  4686000 :  4.1765966719388965\n",
      "Average loss at step  4688000 :  4.1130045312643055\n",
      "Average loss at step  4690000 :  4.112320386648178\n",
      "Average loss at step  4692000 :  4.261409592628479\n",
      "Average loss at step  4694000 :  4.087677291631699\n",
      "Average loss at step  4696000 :  4.153942336678505\n",
      "Average loss at step  4698000 :  4.201802619338036\n",
      "Average loss at step  4700000 :  4.139677202939987\n",
      "Average loss at step  4702000 :  4.140891380667687\n",
      "Average loss at step  4704000 :  4.102851296544075\n",
      "Average loss at step  4706000 :  4.095373595118523\n",
      "Average loss at step  4708000 :  4.1348365542888645\n",
      "Average loss at step  4710000 :  4.092874225378036\n",
      "Average loss at step  4712000 :  4.126527258038521\n",
      "Average loss at step  4714000 :  4.193771615743637\n",
      "Average loss at step  4716000 :  4.252857029199601\n",
      "Average loss at step  4718000 :  4.192071673870086\n",
      "Average loss at step  4720000 :  4.190597433328628\n",
      "Average loss at step  4722000 :  4.075832362413406\n",
      "Average loss at step  4724000 :  4.041760713934899\n",
      "Average loss at step  4726000 :  4.074288045287132\n",
      "Average loss at step  4728000 :  4.041193552017212\n",
      "Average loss at step  4730000 :  4.017028544783592\n",
      "Average loss at step  4732000 :  4.08264250934124\n",
      "Average loss at step  4734000 :  4.196854447960853\n",
      "Average loss at step  4736000 :  4.196469309568405\n",
      "Average loss at step  4738000 :  4.188248085141182\n",
      "Average loss at step  4740000 :  4.187528002619743\n",
      "Average loss at step  4742000 :  4.167995548248291\n",
      "Average loss at step  4744000 :  4.1522647978067395\n",
      "Average loss at step  4746000 :  4.162777234315872\n",
      "Average loss at step  4748000 :  4.205821940660477\n",
      "Average loss at step  4750000 :  4.179435483217239\n",
      "Average loss at step  4752000 :  4.215156222701073\n",
      "Average loss at step  4754000 :  4.174978897690773\n",
      "Average loss at step  4756000 :  4.188460516214371\n",
      "Average loss at step  4758000 :  4.183255469918251\n",
      "Average loss at step  4760000 :  4.245218098640442\n",
      "Average loss at step  4762000 :  4.213205066204071\n",
      "Average loss at step  4764000 :  4.186348322272301\n",
      "Average loss at step  4766000 :  4.152212468266487\n",
      "Average loss at step  4768000 :  4.266079545140267\n",
      "Average loss at step  4770000 :  4.254875265479088\n",
      "Average loss at step  4772000 :  4.2127266790866855\n",
      "Average loss at step  4774000 :  4.1510940370559695\n",
      "Average loss at step  4776000 :  4.07788286459446\n",
      "Average loss at step  4778000 :  3.876075785309076\n",
      "Average loss at step  4780000 :  4.264501018166542\n",
      "Average loss at step  4782000 :  4.196537653326988\n",
      "Average loss at step  4784000 :  4.214475307732821\n",
      "Average loss at step  4786000 :  4.187455269932747\n",
      "Average loss at step  4788000 :  3.927145376086235\n",
      "Average loss at step  4790000 :  4.194821766018867\n",
      "Average loss at step  4792000 :  4.244496875047684\n",
      "Average loss at step  4794000 :  5.489754678249359\n",
      "Average loss at step  4796000 :  4.285159637093544\n",
      "Average loss at step  4798000 :  4.133973937988281\n",
      "Average loss at step  4800000 :  4.0467051619887355\n",
      "Average loss at step  4802000 :  4.197280485033989\n",
      "Average loss at step  4804000 :  4.201058099627494\n",
      "Average loss at step  4806000 :  4.150440886259079\n",
      "Average loss at step  4808000 :  4.119706066727638\n",
      "Average loss at step  4810000 :  4.132005319833755\n",
      "Average loss at step  4812000 :  4.247738929390907\n",
      "Average loss at step  4814000 :  4.169056383490562\n",
      "Average loss at step  4816000 :  4.170945140719414\n",
      "Average loss at step  4818000 :  4.148170567035675\n",
      "Average loss at step  4820000 :  4.178318699002266\n",
      "Average loss at step  4822000 :  4.143513072133064\n",
      "Average loss at step  4824000 :  4.604908114612103\n",
      "Average loss at step  4826000 :  4.370753807425499\n",
      "Average loss at step  4828000 :  4.269444569706917\n",
      "Average loss at step  4830000 :  4.226886239886284\n",
      "Average loss at step  4832000 :  4.187979503631592\n",
      "Average loss at step  4834000 :  4.207112615942955\n",
      "Average loss at step  4836000 :  4.180722655236721\n",
      "Average loss at step  4838000 :  4.193143136739731\n",
      "Average loss at step  4840000 :  4.250371410608292\n",
      "Average loss at step  4842000 :  4.249024580955505\n",
      "Average loss at step  4844000 :  4.19114593732357\n",
      "Average loss at step  4846000 :  4.1908269710540775\n",
      "Average loss at step  4848000 :  4.127516644597054\n",
      "Average loss at step  4850000 :  4.087368236541748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  4852000 :  4.080639743447303\n",
      "Average loss at step  4854000 :  4.074258532762528\n",
      "Average loss at step  4856000 :  4.051882431149482\n",
      "Average loss at step  4858000 :  4.05602584028244\n",
      "Average loss at step  4860000 :  4.051699365735054\n",
      "Average loss at step  4862000 :  4.044718139529229\n",
      "Average loss at step  4864000 :  4.043008219718933\n",
      "Average loss at step  4866000 :  4.080620696663856\n",
      "Average loss at step  4868000 :  4.039088836669922\n",
      "Average loss at step  4870000 :  4.053397094011307\n",
      "Average loss at step  4872000 :  4.043773194551468\n",
      "Average loss at step  4874000 :  4.230286183476448\n",
      "Average loss at step  4876000 :  4.053557598471642\n",
      "Average loss at step  4878000 :  3.9239590253829957\n",
      "Average loss at step  4880000 :  3.9110835117101668\n",
      "Average loss at step  4882000 :  4.092734337806702\n",
      "Average loss at step  4884000 :  4.1930430165827275\n",
      "Average loss at step  4886000 :  4.153760343670845\n",
      "Average loss at step  4888000 :  4.187362653255462\n",
      "Average loss at step  4890000 :  4.0406013233661655\n",
      "Average loss at step  4892000 :  4.0254692990779875\n",
      "Average loss at step  4894000 :  4.108103946566581\n",
      "Average loss at step  4896000 :  4.047164144515992\n",
      "Average loss at step  4898000 :  4.161914081692696\n",
      "Average loss at step  4900000 :  4.171508537530899\n",
      "Average loss at step  4902000 :  4.210466364860535\n",
      "Average loss at step  4904000 :  4.213102500557899\n",
      "Average loss at step  4906000 :  4.1608634793758394\n",
      "Average loss at step  4908000 :  4.138243666648865\n",
      "Average loss at step  4910000 :  3.8955153918266294\n",
      "Average loss at step  4912000 :  4.052295172452927\n",
      "Average loss at step  4914000 :  3.928650547862053\n",
      "Average loss at step  4916000 :  4.070714023470878\n",
      "Average loss at step  4918000 :  4.002773122668266\n",
      "Average loss at step  4920000 :  3.934946298599243\n",
      "Average loss at step  4922000 :  3.9375166553258896\n",
      "Average loss at step  4924000 :  4.101390067815781\n",
      "Average loss at step  4926000 :  4.068477215111256\n",
      "Average loss at step  4928000 :  3.987856806278229\n",
      "Average loss at step  4930000 :  3.998892546534538\n",
      "Average loss at step  4932000 :  3.9774363555908203\n",
      "Average loss at step  4934000 :  4.0949319825172426\n",
      "Average loss at step  4936000 :  4.101275463581085\n",
      "Average loss at step  4938000 :  4.126833439230919\n",
      "Average loss at step  4940000 :  4.161729300260544\n",
      "Average loss at step  4942000 :  4.26969578564167\n",
      "Average loss at step  4944000 :  4.202777359843254\n",
      "Average loss at step  4946000 :  4.16171455258131\n",
      "Average loss at step  4948000 :  4.16707796394825\n",
      "Average loss at step  4950000 :  4.17141627573967\n",
      "Average loss at step  4952000 :  4.058434318423271\n",
      "Average loss at step  4954000 :  4.124613123059273\n",
      "Average loss at step  4956000 :  4.007505760788917\n",
      "Average loss at step  4958000 :  4.060632451415062\n",
      "Average loss at step  4960000 :  4.097709593772888\n",
      "Average loss at step  4962000 :  3.9414310011267664\n",
      "Average loss at step  4964000 :  4.280045745372772\n",
      "Average loss at step  4966000 :  4.283871581315994\n",
      "Average loss at step  4968000 :  4.200398349046707\n",
      "Average loss at step  4970000 :  4.191053079843521\n",
      "Average loss at step  4972000 :  3.9368935475945475\n",
      "Average loss at step  4974000 :  3.9742292066812515\n",
      "Average loss at step  4976000 :  3.899846504330635\n",
      "Average loss at step  4978000 :  3.8726083135008813\n",
      "Average loss at step  4980000 :  3.8945803512334822\n",
      "Average loss at step  4982000 :  3.9318176052570344\n",
      "Average loss at step  4984000 :  3.974111297249794\n",
      "Average loss at step  4986000 :  4.195210456132889\n",
      "Average loss at step  4988000 :  4.199475427627563\n",
      "Average loss at step  4990000 :  4.273524661779404\n",
      "Average loss at step  4992000 :  4.199816401124001\n",
      "Average loss at step  4994000 :  4.468530251860619\n",
      "Average loss at step  4996000 :  4.045087842345238\n",
      "Average loss at step  4998000 :  4.155165011286735\n",
      "Average loss at step  5000000 :  4.156755263328552\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Begin training.\n",
    "data_index = 0\n",
    "num_steps = 5000001\n",
    "# num_steps = 1000001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # Open a writer to write summaries.\n",
    "    writer = tf.summary.FileWriter(FLAGS.log_dir, session.graph)\n",
    "\n",
    "    # We must initialize all variables before we use them.\n",
    "    init.run()\n",
    "    print('Initialized')\n",
    "\n",
    "    average_loss = 0\n",
    "    for step in xrange(num_steps):\n",
    "        batch_inputs, batch_labels = createBatch(batch_size, num_skips,\n",
    "                                                skip_window)\n",
    "#         print(batch_inputs)\n",
    "#         print(batch_labels)\n",
    "#         print(\"**************************\")\n",
    "        \n",
    "        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "\n",
    "        # Define metadata variable.\n",
    "        run_metadata = tf.RunMetadata()\n",
    "\n",
    "        # We perform one update step by evaluating the optimizer op (including it\n",
    "        # in the list of returned values for session.run()\n",
    "        # Also, evaluate the merged op to get all summaries from the returned \"summary\" variable.\n",
    "        # Feed metadata variable to session for visualizing the graph in TensorBoard.\n",
    "        _, summary, loss_val = session.run(\n",
    "            [optimizer, merged, loss],\n",
    "            feed_dict=feed_dict,\n",
    "            run_metadata=run_metadata)\n",
    "        average_loss += loss_val\n",
    "\n",
    "        # Add returned summaries to writer in each step.\n",
    "        writer.add_summary(summary, step)\n",
    "        # Add metadata to visualize the graph for the last run.\n",
    "        if step == (num_steps - 1):\n",
    "            writer.add_run_metadata(run_metadata, 'step%d' % step)\n",
    "        \n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= 2000\n",
    "            # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            print('Average loss at step ', step, ': ', average_loss)\n",
    "            average_loss = 0\n",
    "\n",
    "    final_embeddings = normalized_embeddings.eval()\n",
    "\n",
    "    # Write corresponding labels for the embeddings.\n",
    "    with open(FLAGS.log_dir + '/metadata.tsv', 'w') as f:\n",
    "        for i in xrange(vocabulary_size):\n",
    "            f.write(reverse_dictionary[i] + '\\n')\n",
    "\n",
    "    # Save the model for checkpoints.\n",
    "    saver.save(session, os.path.join(FLAGS.log_dir, 'model.ckpt'))\n",
    "\n",
    "    # Create a configuration for visualizing embeddings with the labels in TensorBoard.\n",
    "    config = projector.ProjectorConfig()\n",
    "    embedding_conf = config.embeddings.add()\n",
    "    embedding_conf.tensor_name = embeddings.name\n",
    "    embedding_conf.metadata_path = os.path.join(FLAGS.log_dir, 'metadata.tsv')\n",
    "    projector.visualize_embeddings(writer, config)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_with_labels(low_dim_embs, labels, filename):\n",
    "    assert low_dim_embs.shape[0] >= len(labels), 'More labels than embeddings'\n",
    "    plt.figure(figsize=(18, 18))  # in inches\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = low_dim_embs[i, :]\n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(\n",
    "            label,\n",
    "            xy=(x, y),\n",
    "            xytext=(5, 2),\n",
    "            textcoords='offset points',\n",
    "            ha='right',\n",
    "            va='bottom')\n",
    "\n",
    "    plt.savefig(filename)\n",
    "\n",
    "\n",
    "try:\n",
    "    from sklearn.manifold import TSNE\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    tsne = TSNE(\n",
    "        perplexity=30, n_components=2, init='pca', n_iter=5000, method='exact')\n",
    "    plot_only = 500\n",
    "    low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only, :])\n",
    "    labels = [reverse_dictionary[i] for i in xrange(plot_only)]\n",
    "    plot_with_labels(low_dim_embs, labels, os.path.join(gettempdir(), 'tsne.png'))\n",
    "\n",
    "except ImportError as ex:\n",
    "    print('Please install sklearn, matplotlib, and scipy to show embeddings.')\n",
    "    print(ex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/parth/tf/lib/python3.5/site-packages/log-tb-4-june'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FLAGS.log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_words(word, final_embeddings, k=10):\n",
    "    word_id = dictionary[word]\n",
    "    similarities = cosine_similarity([final_embeddings[word_id]], final_embeddings)[0]\n",
    "    sort = np.argsort(similarities)[::-1]\n",
    "    probs = []\n",
    "    for x in sort[0:k]:\n",
    "        probs.append((similarities[x], reverse_dictionary[x]))\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_words_close(word, word2, final_embeddings, k=10):\n",
    "    word_id = dictionary[word]\n",
    "    word2_id = dictionary[word2]\n",
    "    similarities = cosine_similarity([final_embeddings[word_id]], [final_embeddings[word2_id]])[0]\n",
    "#     sort = np.argsort(similarities)[::-1]\n",
    "#     probs = []\n",
    "#     for x in sort[0:k]:\n",
    "#         probs.append((similarities[x], reverse_dictionary[x]))\n",
    "    return similarities[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('coquette', 19)\n"
     ]
    }
   ],
   "source": [
    "word_id = dictionary[\"coquette\"]\n",
    "print(count[word_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59260124"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_n_words_close(\"true\",\"false\", final_embeddings, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0, 'grace'),\n",
       " (0.77568376, 'grace,'),\n",
       " (0.6773015, 'heavenly'),\n",
       " (0.6656777, 'perfect'),\n",
       " (0.64709115, 'graces'),\n",
       " (0.6425134, 'gift'),\n",
       " (0.62905186, 'favour'),\n",
       " (0.6281699, 'hand'),\n",
       " (0.6245138, 'beauty,'),\n",
       " (0.6233004, 'grant'),\n",
       " (0.6213466, 'face.'),\n",
       " (0.6208815, 'smiling'),\n",
       " (0.6200605, 'presence'),\n",
       " (0.61119705, 'beauties'),\n",
       " (0.6029942, 'true'),\n",
       " (0.60263026, 'glory'),\n",
       " (0.6017875, 'blessed'),\n",
       " (0.6001467, 'see,'),\n",
       " (0.59968996, 'shew'),\n",
       " (0.5996439, 'honour')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_n_words(\"grace\", final_embeddings, k=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
